{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# set the subset number to use\n",
    "subset = 1\n",
    "\n",
    "# remove index 880 from all since it was a problem data point that the visdial dataloader ignores for some reason: Check tools/functionalties.ipynb for more details\n",
    "\n",
    "'''MASTER NODE IMPLEMENTATION'''\n",
    "\n",
    "'''Load the GAP output of the History Graphs Batch'''\n",
    "with open('../../embeddings/history/' + str(subset) + '/history_batch_GAP.pkl', 'rb') as f:\n",
    "    history_batch_GAP = pickle.load(f)\n",
    "    history_batch_GAP.pop(880)\n",
    "\n",
    "'''Load the GAP output of the Question Graphs Batch'''\n",
    "with open('../../embeddings/fusion/question_GAP_batch_master_node' + str(subset) + '.pkl', 'rb') as f:\n",
    "    question_GAP_batch = pickle.load(f)\n",
    "    question_GAP_batch.pop(880)\n",
    "\n",
    "'''Load the GAP output of the Image Graphs Batch'''\n",
    "with open('../../embeddings/fusion/image_GAP_batch_master_node' + str(subset) + '.pkl', 'rb') as f:\n",
    "    image_GAP_batch = pickle.load(f)\n",
    "    image_GAP_batch.pop(880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the inputs to match the attn_encoder\n",
    "import torch\n",
    "\n",
    "'''Takes in a list of lists of tensors and returns a tensor of tensors: Refactoring the GAP outputs to match the attn_encoder input format'''\n",
    "\n",
    "\n",
    "def adjust_inputs(inputs):\n",
    "    output = []\n",
    "    # flatten the list of lists\n",
    "    inputs = [item for sublist in inputs for item in sublist]\n",
    "    for i in inputs:\n",
    "        i = i.unsqueeze(0)\n",
    "        output.append(i)\n",
    "    output = torch.stack(output)\n",
    "    # convert torch tensor to FloatTensor\n",
    "    output = output.type(torch.FloatTensor)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Input for Attn_Encoder:  torch.Size([80, 1, 512])\n",
      "Question Input for Attn_Encoder:  torch.Size([80, 1, 512])\n",
      "Image Input for Attn_Encoder:  torch.Size([80, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "'''hi: torch.FloatTensor\n",
    "The representation of history utility\n",
    "Shape [batch_size x NH, T, hidden_size]'''\n",
    "hi = DataLoader(adjust_inputs(history_batch_GAP), batch_size=8*10,\n",
    "                shuffle=False)  # note that the batch size is 8 but shaping it to 80 since Dataloader includes the NH dimension\n",
    "\n",
    "'''qe: torch.FloatTensor\n",
    "The representation of question utility\n",
    "Shape [batch_size x NH, N, hidden_size]'''\n",
    "qe = DataLoader(adjust_inputs(question_GAP_batch), batch_size=8*10,\n",
    "                shuffle=False)  # note that the batch size is 8 but shaping it to 80 since Dataloader includes the NH dimension\n",
    "\n",
    "'''im: torch.FloatTensor\n",
    "The representation of image utility\n",
    "Shape [batch_size x NH, K, hidden_size]'''\n",
    "im = DataLoader(adjust_inputs(image_GAP_batch), batch_size=8*10,\n",
    "                shuffle=False)  # note that the batch size is 8 but shaping it to 80 since Dataloader includes the NH dimension\n",
    "\n",
    "for h, q, i in zip(hi, qe, im):\n",
    "    print('History Input for Attn_Encoder: ', h.shape)\n",
    "    print('Question Input for Attn_Encoder: ', q.shape)\n",
    "    print('Image Input for Attn_Encoder: ', i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Mask for Attn_Encoder:  torch.Size([80, 1])\n",
      "Question Mask for Attn_Encoder:  torch.Size([80, 1])\n",
      "Image Mask for Attn_Encoder:  torch.Size([80, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "'''mask_hi: torch.LongTensor\n",
    "Shape [batch_size x NH, T]'''\n",
    "history_masks = torch.ones(\n",
    "    len(history_batch_GAP)*10, 1)  # 1232 different dialogs x 10 number of history rounds\n",
    "mask_hi = DataLoader(history_masks, batch_size=8*10, shuffle=False)\n",
    "\n",
    "'''mask_qe: torch.LongTensor\n",
    "Shape [batch_size x NH, N]'''\n",
    "question_masks = torch.ones(\n",
    "    len(question_GAP_batch)*10, 1)  # 1232 different dialogs x 10 number of history rounds\n",
    "mask_qe = DataLoader(question_masks, batch_size=8*10, shuffle=False)\n",
    "\n",
    "'''mask_im: torch.LongTensor\n",
    "Shape [batch_size x NH, K]'''\n",
    "image_masks = torch.ones(\n",
    "    len(image_GAP_batch)*10, 1)  # 1232 different dialogs x 10 number of history rounds\n",
    "mask_im = DataLoader(image_masks, batch_size=8*10, shuffle=False)\n",
    "\n",
    "for m_h, m_q, m_i in zip(mask_hi, mask_qe, mask_im):\n",
    "    print('History Mask for Attn_Encoder: ', m_h.shape)\n",
    "    print('Question Mask for Attn_Encoder: ', m_q.shape)\n",
    "    print('Image Mask for Attn_Encoder: ', m_i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the config from new_options.py\n",
    "config = {'seed': 42,\n",
    "        'dataset': {\n",
    "                'v0.9': False,\n",
    "                'overfit': False,\n",
    "                'concat_hist': False,\n",
    "                'max_seq_len': 20,\n",
    "                'vocab_min_count': 5,\n",
    "                'finetune': False,\n",
    "                'is_add_boundaries': True,\n",
    "                'is_return_options': True,\n",
    "                'num_boxes': 'fixed',\n",
    "                'glove_path': 'datasets/glove/embedding_Glove_840_300d.pkl',\n",
    "                'train_feat_img_path': 'datasets/bottom-up-attention/trainval_resnet101_faster_rcnn_genome__num_boxes_100_100.h5',\n",
    "                'val_feat_img_path': 'datasets/bottom-up-attention/val2018_resnet101_faster_rcnn_genome__num_boxes_100_100.h5',\n",
    "                'test_feat_img_path': 'datasets/bottom-up-attention/test2018_resnet101_faster_rcnn_genome__num_boxes_100_100.h5',\n",
    "                'train_json_dialog_path': 'datasets/annotations/visdial_1.0_train.json',\n",
    "                'val_json_dialog_path': 'datasets/annotations/visdial_1.0_val.json',\n",
    "                'test_json_dialog_path': 'datasets/annotations/visdial_1.0_test.json',\n",
    "                'val_json_dense_dialog_path': 'datasets/annotations/visdial_1.0_val_dense_annotations.json',\n",
    "                'train_json_word_count_path': 'datasets/annotations/visdial_1.0_word_counts_train.json'\n",
    "                },\n",
    "        'model': {\n",
    "                'decoder_type': 'misc',\n",
    "                'encoder_out': ['img', 'ques'],\n",
    "                'hidden_size': 512,\n",
    "                'dropout': 0.1,\n",
    "                'test_mode': False,\n",
    "\n",
    "                # image features\n",
    "                'img_feat_size': 2048,\n",
    "                'img_num_attns': None,\n",
    "                'img_has_bboxes': False,\n",
    "                'img_has_attributes': False,\n",
    "                'img_has_classes': False,\n",
    "\n",
    "                # text features\n",
    "                'txt_vocab_size': 11322,\n",
    "                'txt_tokenizer': 'nlp',\n",
    "                'txt_bidirectional': True,\n",
    "                'txt_embedding_size': 300,\n",
    "                'txt_has_pos_embedding': False,\n",
    "                'txt_has_layer_norm': False,\n",
    "                'txt_has_decoder_layer_norm': False,\n",
    "\n",
    "                # cross attention\n",
    "                'ca_has_shared_attns': False,\n",
    "                'ca_has_proj_linear': False,\n",
    "                'ca_has_layer_norm': False,\n",
    "                'ca_has_residual': False,\n",
    "                'ca_num_attn_stacks': 1,\n",
    "                'ca_num_attn_heads': 4,\n",
    "                'ca_pad_size': 2,\n",
    "                'ca_has_avg_attns': False,\n",
    "                'ca_has_self_attns': False,\n",
    "                },\n",
    "        'solver': {\n",
    "                # Adam optimizer\n",
    "                'optimizer': 'adam',\n",
    "                'adam_betas': [0.9, 0.997],\n",
    "                'adam_eps': 1e-9,\n",
    "                'weight_decay': 1e-5,\n",
    "                'clip_norm': None,\n",
    "                # dataloader\n",
    "                'num_epochs': 100,\n",
    "                'batch_size': 8,\n",
    "                'cpu_workers': 8,\n",
    "                'batch_size_multiplier': 1,\n",
    "                # learning rate scheduler\n",
    "                'scheduler_type': 'LinearLR',\n",
    "                'init_lr': 5e-3,\n",
    "                'min_lr': 1e-5,\n",
    "                'num_samples': 1233,\n",
    "                # warmup scheduler\n",
    "                'warmup_factor': 0.2,\n",
    "                'warmup_epochs': 1,\n",
    "                # linear scheduler\n",
    "                'linear_gama': 0.5,\n",
    "                'milestone_steps': [3, 6, 8, 10, 11],\n",
    "                'fp16': False,\n",
    "                },\n",
    "        'callbacks': {\n",
    "                'resume': False,\n",
    "                'validate': True,\n",
    "                'path_pretrained_ckpt': None,\n",
    "                'save_dir': 'checkpoints/',\n",
    "                'log_dir': 'checkpoints/tensorboard/',\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.encoders.attn_encoder import AttentionStack\n",
    "'''The Attention Stack include of 3 blocks (i.e. 9 MHAttentions) to compute the attention from all sources to one target (including itself)\n",
    "Attention from X -> Y and Y -> X can be wrapped into a single MultiHeadAttention\n",
    "And self-attention X -> X: can be wrapped into MultiHeadAttention(X, X)'''\n",
    "\n",
    "# initialize the AttentionStack\n",
    "attention_stack = AttentionStack(config)\n",
    "batch_output = []\n",
    "\n",
    "for i, q, h, m_i, m_q, m_h in zip(im, qe, hi, mask_im, mask_qe, mask_hi):\n",
    "    # convert to tuple\n",
    "    batch_input = (i, q, h, m_i, m_q, m_h)\n",
    "    # pass the inputs to the AttentionStack\n",
    "    batch_output.append(attention_stack(batch_input))\n",
    "    # output : A tuples of the updated representations of inputs as the triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train subset] Tokenizing questions...\n",
      "[train subset] Tokenizing answers...\n",
      "[train subset] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "from visdial.data.dataset import VisDialDataset\n",
    "\n",
    "train_dataset = VisDialDataset(config, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_ids', 'num_rounds', 'opts', 'opts_in', 'opts_out', 'opts_len', 'opts_in_len', 'opts_out_len', 'ans', 'ans_in', 'ans_out', 'ans_len', 'ans_in_len', 'ans_out_len', 'ans_ind', 'img_feat', 'ques_tokens', 'hist_tokens', 'ques_len', 'hist_len'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(218703)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['img_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = str(a)\n",
    "# convert string to list\n",
    "c = literal_eval(b)\n",
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=config['solver']['batch_size'],\n",
    "                              num_workers=config['solver']['cpu_workers'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "batch_input = {}\n",
    "for batch, i, q, h, m_i, m_q, m_h in zip(train_dataloader, im, qe, hi, mask_im, mask_qe, mask_hi):\n",
    "    # convert to tuple with batch['img_ids'] as the primary key\n",
    "    batch_input[str(batch['img_ids'].tolist())] = (i, q, h, m_i, m_q, m_h)\n",
    "\n",
    "# save them to disk\n",
    "with open('batch_input.pkl', 'wb') as f:\n",
    "    pickle.dump(batch_input, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "batch_input = pickle.load(open('batch_input.pkl', 'rb'))\n",
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 2, 4, 5]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [3, 1, 2, 4, 5]\n",
    "# sort the list\n",
    "c = sorted(a, key=lambda x: b.index(x))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[218703, 27510, 275130, 166165, 214995, 462664, 114473, 175227]\n",
      "[134001, 547115, 281733, 451345, 122678, 499733, 439352, 26365]\n",
      "[208945, 188771, 240441, 516535, 246467, 338880, 100665, 271283]\n",
      "[22135, 411700, 117192, 301376, 143800, 268718, 554152, 441605]\n",
      "[120118, 282466, 163617, 482493, 58800, 160233, 70133, 500559]\n",
      "[366569, 563371, 425758, 498702, 439926, 435267, 20279, 115158]\n",
      "[175653, 451863, 304238, 147772, 315476, 513766, 206083, 356043]\n",
      "[355902, 366329, 218155, 72860, 459200, 158602, 113787, 550736]\n",
      "[522195, 468337, 504968, 343012, 117325, 316353, 524702, 131101]\n",
      "[143323, 300737, 558915, 496732, 432008, 2843, 221562, 420287]\n",
      "[190547, 228749, 213527, 440093, 82484, 86234, 397379, 433168]\n",
      "[265574, 308520, 217707, 107331, 490810, 116358, 513513, 406508]\n",
      "[121000, 70508, 446832, 224149, 490638, 96535, 317969, 491689]\n",
      "[506220, 301317, 317911, 478982, 123946, 236740, 295563, 120388]\n",
      "[51372, 144608, 452783, 483871, 497455, 515147, 386650, 110138]\n",
      "[445893, 318562, 116466, 347131, 68765, 560683, 273791, 369362]\n",
      "[576810, 437540, 544698, 197264, 157204, 118936, 305724, 503011]\n",
      "[328214, 284465, 431793, 562441, 207785, 72746, 474105, 448076]\n",
      "[491831, 171444, 311904, 335450, 85162, 377760, 228965, 173791]\n",
      "[178410, 147291, 356632, 125672, 505122, 272997, 208549, 61502]\n",
      "[457861, 573113, 357088, 458637, 422956, 283107, 492943, 522416]\n",
      "[472022, 16414, 110265, 468951, 321107, 418318, 537160, 467154]\n",
      "[403830, 487722, 103272, 201738, 145140, 273782, 52802, 118748]\n",
      "[506316, 200597, 465478, 491164, 523487, 363522, 329175, 556130]\n",
      "[115345, 298122, 188854, 526197, 469170, 70370, 463581, 70744]\n",
      "[86220, 270111, 127499, 475763, 37502, 481974, 522946, 289245]\n",
      "[169089, 180083, 170125, 378323, 480196, 480951, 528432, 381154]\n",
      "[246951, 55966, 219943, 231334, 311388, 402903, 133464, 498980]\n",
      "[533312, 261669, 438075, 500401, 186282, 416950, 270270, 355552]\n",
      "[351589, 293362, 116252, 260720, 200446, 226702, 558436, 250677]\n",
      "[423223, 489070, 473590, 100990, 95679, 390094, 132497, 205851]\n",
      "[229127, 130313, 426342, 57753, 16240, 99393, 379488, 388410]\n",
      "[358384, 256549, 426519, 260953, 334809, 176943, 578649, 216098]\n",
      "[35782, 113898, 36758, 534112, 68817, 285733, 328064, 548893]\n",
      "[234900, 215808, 341168, 493956, 420070, 327146, 505876, 257350]\n",
      "[500646, 510405, 483462, 122281, 283170, 143758, 440000, 313694]\n",
      "[23094, 332540, 346862, 336984, 392222, 303431, 127967, 538643]\n",
      "[434039, 187724, 135369, 201596, 352125, 112793, 478474, 62759]\n",
      "[224445, 364939, 90324, 162685, 541643, 187434, 324115, 146819]\n",
      "[475443, 433129, 404428, 481113, 339916, 534555, 469614, 503021]\n",
      "[272658, 473133, 534417, 327337, 270184, 266001, 324483, 405213]\n",
      "[476103, 493509, 573653, 397893, 365251, 536498, 163085, 571541]\n",
      "[536248, 365855, 208071, 289333, 208417, 127659, 104343, 222512]\n",
      "[396051, 207886, 86512, 527118, 71025, 556703, 531617, 574672]\n",
      "[546823, 349393, 125639, 121083, 549650, 200195, 162996, 420412]\n",
      "[244928, 547768, 517772, 108828, 14748, 505789, 430871, 358190]\n",
      "[389232, 209420, 83614, 332627, 369936, 143780, 547072, 167501]\n",
      "[388893, 530466, 506910, 135626, 507384, 231954, 275449, 549377]\n",
      "[434699, 451594, 229813, 577275, 11605, 403531, 128918, 278582]\n",
      "[500981, 253805, 72733, 356641, 452761, 363469, 221433, 482589]\n",
      "[392632, 314750, 484985, 217855, 149783, 570407, 409009, 363136]\n",
      "[158660, 302555, 208782, 484790, 373898, 462709, 296432, 155782]\n",
      "[184916, 513060, 198923, 291669, 365879, 268317, 531404, 324831]\n",
      "[114801, 535460, 74663, 221038, 248341, 267825, 221888, 74335]\n",
      "[140115, 448613, 564355, 467720, 283985, 524069, 129072, 266951]\n",
      "[354460, 70161, 474243, 532036, 106901, 331824, 162055, 370821]\n",
      "[230889, 214888, 51730, 549685, 437546, 320972, 132564, 168574]\n",
      "[101355, 302019, 187795, 301644, 374651, 460461, 482562, 151831]\n",
      "[331780, 385765, 115544, 411617, 218249, 221540, 391696, 156152]\n",
      "[491835, 292529, 271046, 378832, 560242, 468444, 152632, 522252]\n",
      "[131976, 111612, 394244, 524233, 443937, 565166, 230247, 125724]\n",
      "[34975, 341027, 107656, 452004, 426831, 471840, 139958, 255182]\n",
      "[353299, 547391, 158222, 521070, 384617, 207728, 360926, 447099]\n",
      "[461501, 137408, 314112, 19905, 168506, 525369, 579326, 310376]\n",
      "[166297, 544312, 264460, 352850, 38745, 467500, 407992, 511546]\n",
      "[285170, 398878, 461957, 84283, 153965, 470321, 404981, 552304]\n",
      "[323682, 448837, 432150, 361638, 426920, 49713, 515623, 358043]\n",
      "[440475, 448924, 196351, 225360, 273445, 129205, 570788, 371586]\n",
      "[554619, 113147, 553671, 386716, 229522, 536653, 308878, 192120]\n",
      "[567290, 136764, 194034, 109600, 136487, 188100, 377055, 423222]\n",
      "[542036, 344955, 229858, 459320, 162551, 427344, 146249, 129862]\n",
      "[220486, 199072, 77417, 383917, 175347, 569017, 433704, 50695]\n",
      "[86020, 537954, 336102, 246053, 159542, 242992, 496334, 538164]\n",
      "[165231, 227370, 399250, 423480, 129656, 475902, 474680, 180131]\n",
      "[131527, 377393, 325775, 204351, 138079, 235905, 129548, 527908]\n",
      "[413623, 197635, 97496, 315705, 279343, 267612, 571518, 363353]\n",
      "[223701, 216524, 176701, 382089, 439384, 427117, 110415, 104792]\n",
      "[200234, 161854, 472348, 528399, 419473, 457726, 533869, 13797]\n",
      "[277659, 285826, 228318, 133257, 337188, 483645, 566524, 320173]\n",
      "[381199, 578453, 153202, 309467, 76146, 71346, 258000, 560025]\n",
      "[279933, 301912, 481828, 428345, 127229, 335395, 381093, 201042]\n",
      "[116453, 32639, 249325, 423481, 460307, 205440, 300454, 535403]\n",
      "[75270, 476139, 322620, 360600, 322710, 368845, 286770, 248250]\n",
      "[313082, 325410, 518823, 283312, 518948, 316113, 196773, 338458]\n",
      "[136951, 357294, 270956, 172993, 477698, 354933, 409889, 567827]\n",
      "[251047, 550009, 354655, 70675, 292294, 338960, 330253, 284680]\n",
      "[189364, 480469, 264998, 486045, 500723, 321578, 393735, 273772]\n",
      "[496093, 184463, 275791, 547079, 142487, 362503, 229827, 425548]\n",
      "[387124, 405621, 88485, 237315, 81894, 114408, 520389, 493570]\n",
      "[322090, 152788, 535891, 537944, 166678, 277932, 293511, 200447]\n",
      "[205047, 209253, 436538, 225241, 209503, 48304, 517805, 421431]\n",
      "[516342, 224242, 391321, 289318, 478798, 104020, 318924, 528868]\n",
      "[64906, 342004, 500853, 212895, 311179, 252846, 198072, 123411]\n",
      "[548416, 549327, 410498, 310416, 96505, 227941, 260088, 509971]\n",
      "[511271, 140826, 282982, 366740, 367742, 235012, 545363, 291696]\n",
      "[230780, 571695, 266370, 335308, 377284, 293418, 403104, 504517]\n",
      "[174012, 506066, 189351, 237764, 571970, 456608, 391596, 526966]\n",
      "[338850, 138096, 341794, 280133, 444467, 326071, 93141, 493996]\n",
      "[202402, 328008, 202445, 317283, 292421, 364098, 309528, 478670]\n",
      "[504654, 558822, 513968, 129595, 507351, 552721, 581582, 506034]\n",
      "[78305, 113985, 506587, 559642, 554643, 543215, 308276, 545163]\n",
      "[300732, 104602, 397974, 451397, 388228, 201903, 307262, 203846]\n",
      "[406926, 208821, 177924, 515019, 69003, 103585, 106053, 429015]\n",
      "[277764, 293372, 272188, 498418, 477903, 296439, 108688, 490931]\n",
      "[320707, 559614, 301827, 160420, 232890, 94376, 151657, 426101]\n",
      "[311715, 552295, 218716, 572956, 383905, 420612, 110601, 472519]\n",
      "[38083, 207860, 560055, 473354, 576695, 530820, 308772, 335885]\n",
      "[168953, 339796, 246782, 146240, 235870, 117838, 371978, 201873]\n",
      "[580146, 163928, 558054, 480212, 521107, 235741, 434981, 547315]\n",
      "[492774, 266160, 419618, 162033, 439445, 308486, 420303, 545324]\n",
      "[407173, 506872, 381128, 100322, 73835, 289535, 422850, 386426]\n",
      "[273363, 415694, 509553, 577685, 299116, 184626, 554752, 276254]\n",
      "[139660, 385971, 195275, 335660, 484145, 125245, 325208, 472173]\n",
      "[485829, 147392, 124117, 369086, 357362, 139260, 272832, 186130]\n",
      "[343401, 564095, 310419, 212530, 451471, 107800, 176463, 463272]\n",
      "[350017, 457149, 235390, 236530, 483070, 426642, 316336, 470147]\n",
      "[410226, 320759, 293435, 135914, 561729, 386313, 517601, 298242]\n",
      "[508749, 147448, 387206, 261824, 431569, 215584, 402527, 396165]\n",
      "[294698, 185233, 466460, 211811, 25034, 439930, 254136, 499157]\n",
      "[404254, 195812, 565563, 413941, 284144, 103804, 340633, 335344]\n",
      "[230578, 122896, 96222, 87040, 187560, 566886, 493102, 338119]\n",
      "[281164, 227265, 144003, 365027, 384680, 195896, 110786, 488686]\n",
      "[297204, 270179, 81085, 84352, 182096, 552383, 121547, 519566]\n",
      "[108803, 361439, 312958, 360540, 529929, 441470, 417201, 319541]\n",
      "[226976, 549042, 321355, 485173, 121322, 387281, 212553, 575842]\n",
      "[296747, 344422, 50379, 526459, 407607, 423201, 571690, 513980]\n",
      "[316152, 567106, 243324, 348519, 439911, 510548, 575610, 23735]\n",
      "[393867, 403423, 120320, 559838, 518267, 558372, 394131, 334466]\n",
      "[546686, 481861, 264540, 306440, 550934, 331271, 432657, 327802]\n",
      "[381856, 284010, 429580, 165202, 112044, 540388, 451623, 430391]\n",
      "[471175, 430818, 129645, 263008, 176275, 372558, 332905, 491319]\n",
      "[128938, 401330, 303036, 174304, 294159, 350388, 483690, 183597]\n",
      "[254589, 121235, 359110, 126323, 156328, 73104, 509084, 69698]\n",
      "[359436, 440060, 39438, 549300, 486075, 523166, 519607, 537887]\n",
      "[391538, 201723, 168754, 206137, 175604, 101172, 262545, 142891]\n",
      "[371149, 494711, 429462, 249532, 272949, 381887, 499396, 430783]\n",
      "[262986, 476093, 469793, 447233, 33946, 203691, 124927, 108751]\n",
      "[519588, 551692, 14160, 533011, 498949, 230721, 456200, 195755]\n",
      "[573252, 75621, 468354, 552919, 172813, 490336, 490232, 251502]\n",
      "[150989, 259360, 441168, 180857, 325237, 487650, 348111, 306415]\n",
      "[305667, 255519, 228901, 66127, 282738, 104026, 380007, 141517]\n",
      "[290436, 332537, 453801, 313428, 297273, 137211, 365620, 487898]\n",
      "[389772, 220615, 307371, 63740, 572626, 398098, 401804, 162645]\n",
      "[434097, 410735, 528957, 266496, 146504, 410962, 570430, 266209]\n",
      "[294823, 560396, 490434, 365329, 531710, 216279, 476321, 455756]\n",
      "[246265, 275976, 313577, 294238, 489900, 262937, 58316, 235984]\n",
      "[330028, 477061, 351567, 184557, 175477, 461435, 146487, 418506]\n",
      "[226246, 532253, 109653, 444432, 271568, 553078, 579045, 561164]\n",
      "[26147, 573544, 95061, 435343, 485406, 114652, 459835, 202738]\n",
      "[49860, 560359, 406292, 448648, 578391, 324513, 143379, 119543]\n",
      "[102200, 100591, 498903, 301431, 523641, 321557, 549109, 269829]\n",
      "[369580, 451024, 182050, 455505, 210726, 274074, 29519, 338427]\n",
      "[542338, 337552, 413202, 118964, 169262, 404534, 284340, 238748]\n",
      "[287151, 421389, 509948, 561176, 95951, 474224, 472865, 434509]\n"
     ]
    }
   ],
   "source": [
    "for i in batch_input.keys():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.decoders.decoder import Decoder\n",
    "\n",
    "decoder = Decoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.decoders.disc_decoder import DiscriminativeDecoder\n",
    "\n",
    "disc_decoder = DiscriminativeDecoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# initialize the DiscriminativeDecoder output\n",
    "output = []\n",
    "\n",
    "# pass the batch and the batch_output from the Attention Stack to the DiscriminativeDecoder\n",
    "for batch, encoder_outputs in zip(train_dataloader, batch_output):\n",
    "    output.append(decoder(batch, encoder_outputs, True))\n",
    "    print(decoder(batch, encoder_outputs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA number: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from visdial.model import get_model\n",
    "from torch.utils.data import DataLoader\n",
    "from visdial.data.dataset import VisDialDataset\n",
    "from visdial.metrics import SparseGTMetrics, NDCG\n",
    "from visdial.utils.checkpointing import CheckpointManager, load_checkpoint_from_config\n",
    "from visdial.utils import move_to_cuda\n",
    "from visdial.common.utils import check_flag\n",
    "# from new_options import get_training_config_and_args\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from visdial.optim import Adam, LRScheduler, get_weight_decay_params\n",
    "from visdial.loss import DiscLoss\n",
    "\n",
    "# config, args = get_training_config_and_args()\n",
    "\n",
    "seed = config['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "print(f\"CUDA number: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val dataset...\n",
      "[val2018] Tokenizing questions...\n",
      "[val2018] Tokenizing answers...\n",
      "[val2018] Tokenizing captions...\n",
      "Loading train dataset...\n",
      "[train subset] Tokenizing questions...\n",
      "[train subset] Tokenizing answers...\n",
      "[train subset] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DATASET INIT\"\"\"\n",
    "print(\"Loading val dataset...\")\n",
    "val_dataset = VisDialDataset(config, split='val')\n",
    "\n",
    "if check_flag(config['dataset'], 'v0.9'):\n",
    "    val_dataset.dense_ann_feat_reader = None\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=int(config['solver']['batch_size'] /\n",
    "                            2 * torch.cuda.device_count()),\n",
    "                            num_workers=config['solver']['cpu_workers'])\n",
    "\n",
    "print(\"Loading train dataset...\")\n",
    "if config['dataset']['overfit']:\n",
    "    train_dataset = val_dataset\n",
    "    train_dataloader = val_dataloader\n",
    "else:\n",
    "    train_dataset = VisDialDataset(config, split='train')\n",
    "    if check_flag(config['dataset'], 'v0.9'):\n",
    "        train_dataset.dense_ann_feat_reader = None\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=config['solver']['batch_size'] *\n",
    "                                  torch.cuda.device_count(),\n",
    "                                  num_workers=config['solver']['cpu_workers'],\n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_ids', 'num_rounds', 'opts', 'opts_in', 'opts_out', 'opts_len', 'opts_in_len', 'opts_out_len', 'ans', 'ans_in', 'ans_out', 'ans_len', 'ans_in_len', 'ans_out_len', 'ans_ind', 'img_feat', 'ques_tokens', 'hist_tokens', 'ques_len', 'hist_len'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model...\n",
      "Cuda available, using gpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"MODEL INIT\"\"\"\n",
    "print(\"Init model...\")\n",
    "model = get_model(config)\n",
    "# set device to cpu if cuda is not available\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Cuda not available, using cpu\")\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Cuda available, using gpu\")\n",
    "model = get_model(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LOSS FUNCTION\"\"\"\n",
    "from visdial.loss import DiscLoss\n",
    "\n",
    "disc_criterion = DiscLoss(return_mean=True)\n",
    "gen_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\"\"\"OPTIMIZER\"\"\"\n",
    "parameters = get_weight_decay_params(\n",
    "    model, weight_decay=config['solver']['weight_decay'])\n",
    "\n",
    "optimizer = Adam(parameters,\n",
    "                 betas=config['solver']['adam_betas'],\n",
    "                 eps=config['solver']['adam_eps'],\n",
    "                 weight_decay=config['solver']['weight_decay'])\n",
    "\n",
    "lr_scheduler = LRScheduler(optimizer,\n",
    "                           batch_size=config['solver']['batch_size'] *\n",
    "                           torch.cuda.device_count(),\n",
    "                           num_samples=config['solver']['num_samples'],\n",
    "                           num_epochs=config['solver']['num_epochs'],\n",
    "                           min_lr=config['solver']['min_lr'],\n",
    "                           init_lr=config['solver']['init_lr'],\n",
    "                           warmup_factor=config['solver']['warmup_factor'],\n",
    "                           warmup_epochs=config['solver']['warmup_epochs'],\n",
    "                           scheduler_type=config['solver']['scheduler_type'],\n",
    "                           milestone_steps=config['solver']['milestone_steps'],\n",
    "                           linear_gama=config['solver']['linear_gama']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoints...\n",
      "Can't load weight from None\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#   SETUP BEFORE TRAINING LOOP\n",
    "# =============================================================================\n",
    "summary_writer = SummaryWriter(log_dir=config['callbacks']['log_dir'])\n",
    "\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    model, optimizer, config['callbacks']['save_dir'], config=config)\n",
    "sparse_metrics = SparseGTMetrics()\n",
    "disc_metrics = SparseGTMetrics()\n",
    "gen_metrics = SparseGTMetrics()\n",
    "ndcg = NDCG()\n",
    "disc_ndcg = NDCG()\n",
    "gen_ndcg = NDCG()\n",
    "\n",
    "print(\"Loading checkpoints...\")\n",
    "start_epoch, model, optimizer = load_checkpoint_from_config(\n",
    "    model, optimizer, config)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs and DataParallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#   TRAINING LOOP\n",
    "# =============================================================================\n",
    "iterations = len(\n",
    "    train_dataset) // (config['solver']['batch_size'] * torch.cuda.device_count()) + 1\n",
    "num_examples = torch.tensor(len(train_dataset), dtype=torch.float)\n",
    "global_iteration_step = start_epoch * iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 0:\n",
      "Training for epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/155 [00:03<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m \u001b[39m# do forward\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m out \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     17\u001b[0m \u001b[39m# compute loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m gen_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gnnVD/github/visdial/visdial/model.py:26\u001b[0m, in \u001b[0;36mVisdialModel.forward\u001b[0;34m(self, batch, test_mode)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch, test_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(batch, test_mode\u001b[39m=\u001b[39;49mtest_mode), test_mode\u001b[39m=\u001b[39mtest_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gnnVD/github/visdial/visdial/encoders/encoder.py:81\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, batch, test_mode)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mArguments\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m---------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m    N is the max sequence length in the question.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# [BS x NH, T, HS] hist\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m# [BS x NH, N, HS] ques\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# [BS x NH, T] hist_mask\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# [BS x NH, N] ques_mask\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m hist, ques, hist_mask, ques_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder(batch, test_mode\u001b[39m=\u001b[39;49mtest_mode)\n\u001b[1;32m     83\u001b[0m \u001b[39m# [BS x NH, K, HS] img\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m# [BS x NH, K] img_mask\u001b[39;00m\n\u001b[1;32m     85\u001b[0m img, img_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_encoder(batch, test_mode\u001b[39m=\u001b[39mtest_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gnnVD/github/visdial/visdial/encoders/text_encoder.py:60\u001b[0m, in \u001b[0;36mTextEncoder.forward\u001b[0;34m(self, batch, test_mode)\u001b[0m\n\u001b[1;32m     54\u001b[0m hist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_embedding(hist_tokens)\n\u001b[1;32m     56\u001b[0m \u001b[39m# ques: shape [BS x NH, N, hidden_size]\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# hist: shape [BS x NH, num_rounds, hidden_size]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# ques_mask: shape [BS x NH, N]\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# hist_mask: shape [BS x NH, num_rounds]\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m ques, ques_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mques_encoder(ques, ques_len, test_mode\u001b[39m=\u001b[39;49mtest_mode)\n\u001b[1;32m     61\u001b[0m hist, hist_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_encoder(hist, hist_len, test_mode\u001b[39m=\u001b[39mtest_mode)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m hist, ques, hist_mask, ques_mask\n",
      "File \u001b[0;32m~/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gnnVD/github/visdial/visdial/encoders/text_encoder.py:141\u001b[0m, in \u001b[0;36mQuesEncoder.forward\u001b[0;34m(self, ques, ques_len, test_mode)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m ques, ques_mask\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    138\u001b[0m \u001b[39m# BiLSTM\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[39m# [BS x NH, SEQ, HS x 2]\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     ques, (_, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mques_lstm(ques, ques_len)\n\u001b[1;32m    143\u001b[0m     \u001b[39m# [BS x NH, SEQ, HS]\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     ques \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mques_linear(ques)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/gnnVD/github/visdial/visdial/common/dynamic_rnn.py:51\u001b[0m, in \u001b[0;36mDynamicRNN.forward\u001b[0;34m(self, x, len_x, initial_state)\u001b[0m\n\u001b[1;32m     48\u001b[0m sorted_x \u001b[39m=\u001b[39m x[idx]\n\u001b[1;32m     50\u001b[0m \u001b[39m# Convert to packed sequence batch\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m packed_x \u001b[39m=\u001b[39m pack_padded_sequence(sorted_x, lengths\u001b[39m=\u001b[39;49msorted_len, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39m# Check init_state\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/nn/utils/rnn.py:260\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    256\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    257\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[1;32m    259\u001b[0m data, batch_sizes \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 260\u001b[0m     _VF\u001b[39m.\u001b[39;49m_pack_padded_sequence(\u001b[39minput\u001b[39;49m, lengths, batch_first)\n\u001b[1;32m    261\u001b[0m \u001b[39mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, config['solver']['num_epochs']):\n",
    "    print(f\"Training for epoch {epoch}:\")\n",
    "    print(f\"Training for epoch {epoch}:\")\n",
    "    if check_flag(config['dataset'], 'v0.9') and epoch > 6:\n",
    "        break\n",
    "\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for batch in tqdm(train_dataloader, total=iterations, unit=\"batch\"):\n",
    "        batch = move_to_cuda(batch, device)\n",
    "\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do forward\n",
    "        out = model(batch)\n",
    "\n",
    "        # compute loss\n",
    "        gen_loss = torch.tensor(0.0, requires_grad=True, device='cuda')\n",
    "        disc_loss = torch.tensor(0.0, requires_grad=True, device='cuda')\n",
    "        batch_loss = torch.tensor(0.0, requires_grad=True, device='cuda')\n",
    "        if out.get('opt_scores') is not None:\n",
    "            scores = out['opt_scores'].view(-1, 100)\n",
    "            target = batch['ans_ind'].view(-1)\n",
    "\n",
    "            sparse_metrics.observe(out['opt_scores'], batch['ans_ind'])\n",
    "            disc_loss = disc_criterion(scores, target)\n",
    "            batch_loss = batch_loss + disc_loss\n",
    "\n",
    "        if out.get('ans_out_scores') is not None:\n",
    "            scores = out['ans_out_scores'].view(-1,\n",
    "                                                config['model']['txt_vocab_size'])\n",
    "            target = batch['ans_out'].view(-1)\n",
    "            gen_loss = gen_criterion(scores, target)\n",
    "            batch_loss = batch_loss + gen_loss\n",
    "\n",
    "        # compute gradients\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update params\n",
    "        lr = lr_scheduler.step(global_iteration_step)\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        if config['dataset']['overfit']:\n",
    "            print(\"epoch={:02d}, steps={:03d}K: batch_loss:{:.03f} \"\n",
    "                  \"disc_loss:{:.03f} gen_loss:{:.03f} lr={:.05f}\".format(\n",
    "                      epoch, int(global_iteration_step /\n",
    "                                 1000), batch_loss.item(),\n",
    "                      disc_loss.item(), gen_loss.item(), lr))\n",
    "\n",
    "        if global_iteration_step % 1000 == 0:\n",
    "            print(\"epoch={:02d}, steps={:03d}K: batch_loss:{:.03f} \"\n",
    "                  \"disc_loss:{:.03f} gen_loss:{:.03f} lr={:.05f}\".format(\n",
    "                      epoch, int(global_iteration_step /\n",
    "                                 1000), batch_loss.item(),\n",
    "                      disc_loss.item(), gen_loss.item(), lr))\n",
    "\n",
    "        summary_writer.add_scalar(config['config_name'] + \"-train/batch_loss\",\n",
    "                                  batch_loss.item(), global_iteration_step)\n",
    "        summary_writer.add_scalar(\"train/batch_lr\", lr, global_iteration_step)\n",
    "\n",
    "        global_iteration_step += 1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_loss += batch[\"ans\"].size(0) * batch_loss.detach()\n",
    "\n",
    "    if out.get('opt_scores') is not None:\n",
    "        avg_metric_dict = {}\n",
    "        avg_metric_dict.update(sparse_metrics.retrieve(reset=True))\n",
    "\n",
    "        summary_writer.add_scalars(config['config_name'] + \"-train/metrics\",\n",
    "                                   avg_metric_dict, global_iteration_step)\n",
    "\n",
    "        for metric_name, metric_value in avg_metric_dict.items():\n",
    "            print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    epoch_loss /= num_examples\n",
    "    summary_writer.add_scalar(config['config_name'] + \"-train/epoch_loss\",\n",
    "                              epoch_loss.item(), global_iteration_step)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #   ON EPOCH END  (checkpointing and validation)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Validate and report automatic metrics.\n",
    "\n",
    "    if config['callbacks']['validate']:\n",
    "        # Switch dropout, batchnorm etc to the correct mode.\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"\\nValidation after epoch {epoch}:\")\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            move_to_cuda(batch, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(batch)\n",
    "\n",
    "                if out.get('opt_scores') is not None:\n",
    "                    scores = out['opt_scores']\n",
    "                    disc_metrics.observe(scores, batch[\"ans_ind\"])\n",
    "\n",
    "                    if \"gt_relevance\" in batch:\n",
    "                        scores = scores[\n",
    "                            torch.arange(scores.size(0)),\n",
    "                            batch[\"round_id\"] - 1, :]\n",
    "\n",
    "                        disc_ndcg.observe(scores, batch[\"gt_relevance\"])\n",
    "\n",
    "                if out.get('opts_out_scores') is not None:\n",
    "                    scores = out['opts_out_scores']\n",
    "                    gen_metrics.observe(scores, batch[\"ans_ind\"])\n",
    "\n",
    "                    if \"gt_relevance\" in batch:\n",
    "                        scores = scores[\n",
    "                            torch.arange(scores.size(0)),\n",
    "                            batch[\"round_id\"] - 1, :]\n",
    "\n",
    "                        gen_ndcg.observe(scores, batch[\"gt_relevance\"])\n",
    "\n",
    "                if out.get('opt_scores') is not None and out.get('opts_out_scores') is not None:\n",
    "                    scores = (out['opts_out_scores'] + out['opt_scores']) / 2\n",
    "\n",
    "                    sparse_metrics.observe(scores, batch[\"ans_ind\"])\n",
    "                    if \"gt_relevance\" in batch:\n",
    "                        scores = scores[\n",
    "                            torch.arange(scores.size(0)),\n",
    "                            batch[\"round_id\"] - 1, :]\n",
    "\n",
    "                        ndcg.observe(scores, batch[\"gt_relevance\"])\n",
    "\n",
    "        avg_metric_dict = {}\n",
    "        avg_metric_dict.update(sparse_metrics.retrieve(reset=True, key='avg_'))\n",
    "        avg_metric_dict.update(ndcg.retrieve(reset=True, key='avg_'))\n",
    "\n",
    "        disc_metric_dict = {}\n",
    "        disc_metric_dict.update(disc_metrics.retrieve(reset=True, key='disc_'))\n",
    "        disc_metric_dict.update(disc_ndcg.retrieve(reset=True, key='disc_'))\n",
    "\n",
    "        gen_metric_dict = {}\n",
    "        gen_metric_dict.update(gen_metrics.retrieve(reset=True, key='gen_'))\n",
    "        gen_metric_dict.update(gen_ndcg.retrieve(reset=True, key='gen_'))\n",
    "\n",
    "        for metric_dict in [avg_metric_dict, disc_metric_dict, gen_metric_dict]:\n",
    "            for metric_name, metric_value in metric_dict.items():\n",
    "                print(f\"{metric_name}: {metric_value}\")\n",
    "            summary_writer.add_scalars(config['config_name'] + \"-val/metrics\",\n",
    "                                       metric_dict, global_iteration_step)\n",
    "\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Checkpoint\n",
    "        if not config['dataset']['overfit']:\n",
    "            if 'disc' in config['model']['decoder_type']:\n",
    "                checkpoint_manager.step(\n",
    "                    epoch=epoch, only_best=False, metrics=disc_metric_dict, key='disc_')\n",
    "\n",
    "            elif 'gen' in config['model']['decoder_type']:\n",
    "                checkpoint_manager.step(\n",
    "                    epoch=epoch, only_best=False, metrics=gen_metric_dict, key='gen_')\n",
    "\n",
    "            elif 'misc' in config['model']['decoder_type']:\n",
    "                checkpoint_manager.step(\n",
    "                    epoch=epoch, only_best=False, metrics=disc_metric_dict, key='disc_')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnVD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
