{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what color is the middle vase?',\n",
       " 'are there people going into the buildings?',\n",
       " 'is the person walking on a sidewalk?',\n",
       " 'what is wallpaper like?',\n",
       " 'does the fluid cover the bananas?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This is strictly for the 100 percent data set questions\n",
    "Reason: The LAL-parser crashes at around 39% when tagging the questions on the whole train set when the questions have been initialized with a '?' symbol at the end. To fix this, we remove the '?' symbol from the questions and then tag them. Afterwards, we add the 'punct' token to the dependency labels and the index of the root node to the dependency heads'''\n",
    "import json\n",
    "\n",
    "'''First read the Questions'''\n",
    "with open('../embeddings/questions/100/questions.txt', 'r') as f:\n",
    "    questions = f.readlines()\n",
    "    questions = [q.strip() for q in questions] # remove the newline character\n",
    "\n",
    "# show the first 5 questions\n",
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6, 6, 6, 6, 0],\n",
       " [4, 4, 4, 0, 4, 7, 5],\n",
       " [4, 3, 4, 0, 4, 7, 5],\n",
       " [0, 4, 4, 1],\n",
       " [4, 3, 4, 0, 6, 4]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here we read the Dependency Heads'''\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "with open('../embeddings/questions/100/dependency_heads.txt', 'r') as f:\n",
    "    dep_head_list = f.readlines()\n",
    "    dep_head_list = [element.strip() for element in dep_head_list]\n",
    "    dep_head_list = [ast.literal_eval(element) for element in dep_head_list]\n",
    "\n",
    "dep_head_list[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['det', 'dep', 'cop', 'det', 'amod', 'nsubj', 'punct'],\n",
       " ['aux', 'nsubj', 'nsubj', 'root', 'prep', 'det', 'pobj', 'punct'],\n",
       " ['aux', 'det', 'nsubj', 'root', 'prep', 'det', 'pobj', 'punct'],\n",
       " ['dep', 'cop', 'nsubj', 'prep', 'punct'],\n",
       " ['aux', 'det', 'nsubj', 'root', 'det', 'dobj', 'punct']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Similarly, we read the Dependency Labels'''\n",
    "with open('../embeddings/questions/100/dependency_labels.txt', 'r') as f:\n",
    "    dep_label_list = f.readlines()\n",
    "    dep_label_list = [element.strip() for element in dep_label_list]\n",
    "    dep_label_list = [ast.literal_eval(element) for element in dep_label_list]\n",
    "    # add the element 'punct' to the each list of dependency labels\n",
    "    dep_label_list = [dep_label + ['punct'] for dep_label in dep_label_list]\n",
    "\n",
    "dep_label_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376082/376082 [3:28:43<00:00, 30.03it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 6, 6, 6, 6, 0, 2],\n",
       " [4, 4, 4, 0, 4, 7, 5, 1],\n",
       " [4, 3, 4, 0, 4, 7, 5, 4],\n",
       " [0, 4, 4, 1, 1],\n",
       " [4, 3, 4, 0, 6, 4, 4]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "from tqdm import tqdm\n",
    "\n",
    "# initialize the stanza pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, lemma, depparse', verbose=False, use_gpu=True, pos_batch_size=10, lemma_batch_size=10, depparse_batch_size=10)\n",
    "\n",
    "'''Find the index of the root of a sentence using the Stanza parser'''\n",
    "def find_root_index(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.deprel == 'root':\n",
    "                return word.id\n",
    "\n",
    "\n",
    "# add the root index to the dependency heads\n",
    "dep_head_list = [dep_head + [find_root_index(question)]\n",
    "                 for dep_head, question in zip(tqdm(dep_head_list), questions)]\n",
    "dep_head_list[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dependency heads and the dependency labels to a text file\n",
    "with open('../embeddings/questions/100/dependency_labels.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    for dep_label in dep_label_list:\n",
    "        myfile.write(str(dep_label) + '\\n')\n",
    "#    myfile.write('\\n'.join(dep_label_list))\n",
    "\n",
    "with open('../embeddings/questions/100/dependency_heads.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    for dep_head in dep_head_list:\n",
    "        myfile.write(str(dep_head) + '\\n')\n",
    "#    myfile.write('\\n'.join(dep_head_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnnVD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
