{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# set the data subset to use\n",
    "subset = 1\n",
    "# load data\n",
    "data = json.load(open('../data/subsets/visdial_1.0_train_'+ str(subset) +'percent_subset.json'))['data']\n",
    "# load questions\n",
    "questions = data['questions']\n",
    "# load answers\n",
    "answers = data['answers']\n",
    "# load history: captions of images and the dialog rounds\n",
    "dialogs = data['dialogs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note: While doing the History Graph modeling,\n",
    "Node 0 -> Caption\n",
    "Node 1:11 -> Question-Answer rounds with Question-and-<Answered Answer> Pairs'''\n",
    "\n",
    "# storing the history\n",
    "history = []\n",
    "\n",
    "for index in range(len(dialogs)):\n",
    "    # storing the image caption\n",
    "    caption = dialogs[index]['caption']\n",
    "    # storing each of the dialog rounds\n",
    "    q_and_a = []\n",
    "    # for the dialog rounds\n",
    "    for rounds in range(10):\n",
    "        # question key and the question\n",
    "        question_key = dialogs[index]['dialog'][rounds]['question']\n",
    "        question = questions[question_key]\n",
    "        # answer key and the selected answer\n",
    "        answer_key = dialogs[index]['dialog'][rounds]['answer']\n",
    "        answer = answers[answer_key]\n",
    "        # append to q_and_a: note that the questions and answers have been separated by a \"? \" to mimic the GOG history features structure\n",
    "        q_and_a.append(\"? \".join([question, answer]))\n",
    "    # add the caption to the zero'th position for continuity (doesn't really matter since Graphs preserve the relational structure between the nodes)\n",
    "    q_and_a.insert(0, caption)\n",
    "    # append to history\n",
    "    history.append(q_and_a)\n",
    "\n",
    "# save the history structure for coreference resolution\n",
    "with open('../embeddings/history/'+ str(subset) +'/history.json', 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    # if the data is nested\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a sailboat-carrying truck with people in the back on a tree lined road',\n",
       " \"are they pulled back? yes it's inside a phone case made to look like a plastic green shoe\",\n",
       " \"so he's out there with some other fellow surfers maybe? lettuce and carrot shreds\",\n",
       " 'are people walking on the platform? framed photo',\n",
       " 'is the bridge for vehicles or for people to walk over? it is very clean',\n",
       " 'is there any people around pizza? 8 or 9',\n",
       " 'is there toilet paper on a roll? black top, white wheels',\n",
       " 'where would you say this is if you had to guess? there is another blurry giraffe in the background',\n",
       " 'what is the seat color? throwing frisbee',\n",
       " 'are his socks pulled up to his knees? all i see is some hand soap',\n",
       " 'is this professional or amateur photo? black top, white wheels']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# set the data subset to use\n",
    "subset = 1\n",
    "\n",
    "'''Load the saved history structure'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/history.json', 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "# example of a history entry\n",
    "history[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neuralcoref\n",
    "import spacy\n",
    "\n",
    "# Load the Spacy model and add neural coref to SpaCy's pipe\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "'''Takes two strings and returns a boolean value indicating whether they are coreferent or not\n",
    "    Method: Uses the neuralcoref library to compute the coref_scores for the document containing the two strings\n",
    "            doc._.coref_scores is a dictionary with keys as the coreferent entities and a dict of all the coreferent entities and their scores as the values\n",
    "            Get a list of all values in this dict_list that are positive:\n",
    "                Return true if among these, doc._.coref_scores.keys() and doc._.coref_score entities are not in the same strings''' \n",
    "\n",
    "def is_coreferent(string1, string2):\n",
    "    # get the document\n",
    "    doc = nlp(string1 + \" \" + string2)\n",
    "    # get the coref_scores\n",
    "    coref_scores = doc._.coref_scores\n",
    "    # get the list of all values in this dict_list that are positive\n",
    "    for keys, values in coref_scores.items():\n",
    "        for key, value in values.items():\n",
    "            if value > 0:\n",
    "                # check if the first element is from set 1 and third is from set 2 or vice versa\n",
    "                if (keys.text in string1 and key.text in string2) or (keys.text in string2 and key.text in string1):\n",
    "                    return True\n",
    "\n",
    "# is_coreferent(history[0][0], history[0][1])\n",
    "is_coreferent(\"My mother's name is Sanchita.\", \"She likes my home-cooked food.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loop through all history entries and compute the edge list and the adj list for each history entry'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "edge_list = [] # initialize the edge list for all history entries\n",
    "for entry in tqdm(history):\n",
    "    edge_list_entry = [] # initialize the edge list for each history entry\n",
    "    compare_list = [entry[0]] # initialize the compare list for each history entry initialized with the caption\n",
    "    for index in entry[1:]:\n",
    "        for compare in compare_list:\n",
    "            if is_coreferent(index, compare):\n",
    "                # an edge between two nodes is indicated by both (0, 1) and (1, 0) since the graph is undirected\n",
    "                edge_list_entry.append((entry.index(index), entry.index(compare))) \n",
    "                edge_list_entry.append((entry.index(compare), entry.index(index))) \n",
    "        compare_list.append(index)\n",
    "    edge_list.append(edge_list_entry)\n",
    "\n",
    "\n",
    "with open('../embeddings/history/'+ str(subset) +'/edge_list.json', 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    # if the data is nested\n",
    "    json.dump(edge_list, f)\n",
    "\n",
    "adj_list = []\n",
    "for element in tqdm(edge_list):\n",
    "    # size here is 11 since there are always 11 nodes in each history entry\n",
    "    size = 11\n",
    "    # initialize adjacency list\n",
    "    adj_list_element = [[0 for i in range(size)] for j in range(size)]\n",
    "    # add edges to the adjacency list\n",
    "    if len(element) != 0:\n",
    "        for row, col in element:\n",
    "            adj_list_element[row][col] = 1 # also an undirected edge since the edge list has both (0, 1) and (1, 0)\n",
    "    # add the adjacency list to the list of adjacency matrices\n",
    "    adj_list.append(adj_list_element)\n",
    "\n",
    "with open('../embeddings/history/'+ str(subset) +'/adj_list.json', 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    # if the data is nested\n",
    "    json.dump(adj_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 1], [2, 1], [1, 2], [3, 1], [1, 3], [4, 0], [0, 4], [4, 1], [1, 4], [4, 2], [2, 4], [4, 3], [3, 4], [5, 1], [1, 5], [5, 4], [4, 5], [6, 1], [1, 6], [6, 4], [4, 6], [7, 2], [2, 7], [9, 1], [1, 9], [9, 2], [2, 9], [9, 4], [4, 9], [9, 7], [7, 9], [10, 1], [1, 10], [10, 4], [4, 10], [10, 6], [6, 10], [10, 7], [7, 10], [10, 9], [9, 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       " [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       " [0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# set the data subset to use\n",
    "subset = 1\n",
    "\n",
    "'''Displaying the first edge list'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/edge_list.json', 'r') as f:\n",
    "    edge_list = json.load(f)\n",
    "\n",
    "# example of an edge list entry\n",
    "print(edge_list[0])\n",
    "\n",
    "'''Displaying the first adjacency matrix'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/adj_list.json', 'r') as f:\n",
    "    adj_list = json.load(f)\n",
    "\n",
    "# example of an adjacency matrix entry\n",
    "adj_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove-6b-300d Model:\n",
      "Done. Glove-6b-300d with a vocabulary of 399998 words was loaded!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    readable_name = \"-\".join(glove_file.rstrip('.txt').split('/')[-1].split(\".\")).capitalize()\n",
    "    print(f\"Loading {readable_name} Model:\")\n",
    "    df = pd.read_csv(glove_file, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    glove_model = {key: val.values for key, val in df.T.items()}\n",
    "    print(f\"Done. {readable_name} with a vocabulary of {len(glove_model)} words was loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_6b_300 = load_glove_model('../embeddings/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Tokenize a sentence using a glove model, pass it through an LSTm and return the last LSTM hidden state'''\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def sentence_LSTM(glove_model, sentence):\n",
    "    # tokenize the sentence\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # to remove punctuations\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence.lower())\n",
    "\n",
    "    # get the glove embedding for each word in the sentence\n",
    "    glove_embeddings = []\n",
    "    for word in tokenized_sentence:\n",
    "        if word in glove_model.keys():\n",
    "            glove_embeddings.append(glove_model[word])\n",
    "        else:\n",
    "            glove_embeddings.append(glove_model['unk'])\n",
    "\n",
    "    # convert the glove embeddings to a numpy array\n",
    "    glove_embeddings = np.array(glove_embeddings)\n",
    "    # convert the glove embeddings to a tensor\n",
    "    glove_embeddings = torch.tensor(glove_embeddings).float()\n",
    "\n",
    "    # pass the glove embeddings through an LSTM\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "    lstm = nn.LSTM(input_size=300, hidden_size=300,\n",
    "                   num_layers=1, batch_first=True)\n",
    "    lstm_out, (h_n, c_n) = lstm(\n",
    "        glove_embeddings.view(1, len(tokenized_sentence), 300))\n",
    "\n",
    "    # return the last hidden state\n",
    "    return h_n.squeeze()\n",
    "\n",
    "'''Testing LSTM hidden state output'''\n",
    "sentence_LSTM(glove_6b_300, \"What is the color of the shirt?\").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [01:17<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "history_LSTM = []\n",
    "'''Each entry in the History LSTM contains a list of the LSTM hidden state for each history node entry of a round of dialog'''\n",
    "for entry in tqdm(history):\n",
    "    history_LSTM.append(torch.tensor(np.array([sentence_LSTM(glove_6b_300, index).detach().numpy() for index in entry])))\n",
    "\n",
    "# save the history_LSTM in a pickle file\n",
    "with open('../embeddings/history/'+ str(subset) +'/history_LSTM.pkl', 'wb') as f:\n",
    "    pickle.dump(history_LSTM, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_LSTM[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# set the subset\n",
    "subset = 1\n",
    "\n",
    "'''Load the edge list'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/edge_list.json', 'r') as f:\n",
    "    edge_list = json.load(f)\n",
    "\n",
    "'''Load the adjacency matrix'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/adj_list.json', 'r') as f:\n",
    "    adj_list = json.load(f)\n",
    "\n",
    "'''Load the history_LSTM'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/history_LSTM.pkl', 'rb') as f:\n",
    "    history_LSTM = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [00:00<00:00, 13138.97it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Intialize the graph with pytorch geometric'''\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''Create a list of Data objects for each question where:\n",
    "    - x is the history_LSTM\n",
    "    Note: Each history_LSTM is a list of the LSTM hidden states for each history node entry of a round of dialog\n",
    "    - edge_index is the edge list where they are contiguous'''\n",
    "\n",
    "history_graphs = []\n",
    "for i in tqdm(range(len(history_LSTM))):\n",
    "    history_graphs.append(Data(x=torch.FloatTensor(np.array(history_LSTM[i])),\n",
    "                        edge_index=torch.LongTensor(np.array(edge_list[i])).t().contiguous()))\n",
    "\n",
    "'''Save the Data objects to a file'''\n",
    "import pickle\n",
    "\n",
    "with open('../embeddings/history/'+ str(subset) +'/history_graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(history_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrView(store=Data(x=[11, 300], edge_index=[2, 42]), attr=DataTensorAttr(group_name=None, attr_name=<FieldStatus.UNSET: 1>, index=<FieldStatus.UNSET: 1>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load the history_graphs\n",
    "with open('../embeddings/history/'+ str(subset) +'/history_graphs.pkl', 'rb') as f:\n",
    "    history_graphs = pickle.load(f)\n",
    "\n",
    "history_graphs[0].view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Visualize the first history graph'''\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert a pytorch geometric graph to a networkx graph\n",
    "def visualize_graph(graph):\n",
    "    G = to_networkx(graph)\n",
    "    # initialize the figure and axes | Note: This somehow gets around the \"TypeError: '_AxesStack' object is not callable\"\n",
    "    fig, ax = plt.subplots() \n",
    "    # draw the graph\n",
    "    nx.draw(G, ax=ax, with_labels=True)\n",
    "    # set the figure size 4x4\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "visualize_graph(history_graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(\n",
    "            num_heads, 2 * c_out))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "        node_feats_flat = node_feats.view(\n",
    "            batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat,\n",
    "                               index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat,\n",
    "                               index=edge_indices_col, dim=0)\n",
    "        ], dim=-1)  # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(\n",
    "            adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(\n",
    "            1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 300])\n",
      "torch.Size([11, 512])\n"
     ]
    }
   ],
   "source": [
    "'''Initialize the GAT layer with:\n",
    "- input dimension of 300 (the dimensionality of the node features)\n",
    "- output dimension of 512 (the dimensionality of the output features)\n",
    "- 4 attention heads\n",
    "- attention heads are not concatenated\n",
    "- alpha is set to 0.2\n",
    "Note: The features here are set from the GoG Paper'''\n",
    "\n",
    "gat_layer = GATLayer(c_in=300, c_out=512, num_heads=4, concat_heads=False, alpha=0.2)\n",
    "# print(history_graphs[0].x.shape)\n",
    "# print(torch.tensor(adj_list[0]).shape)\n",
    "\n",
    "# testing the GAT layer with the first history graph\n",
    "with torch.no_grad():    \n",
    "    # note that we have to unsqueeze the node features to fit the GAT input shape\n",
    "    out_feats = gat_layer(history_graphs[0].x.unsqueeze(0), torch.tensor([adj_list[0]]), print_attn_probs=False)\n",
    "    \n",
    "print(history_graphs[0].x.squeeze().shape)\n",
    "print(out_feats.squeeze().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [00:00<00:00, 1363.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''Do this for all History Graphs and save it to the History Graph Attention List'''\n",
    "history_gat = history_graphs.copy()\n",
    "for i in tqdm(range(len(history_graphs))):\n",
    "    with torch.no_grad():\n",
    "        out = gat_layer(history_graphs[i].x.unsqueeze(0), torch.tensor([adj_list[i]]), print_attn_probs=False)\n",
    "    history_gat[i].x = out.squeeze()\n",
    "\n",
    "'''Save the History Graph Attention List to a pickle file in the proper directory'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/history_GAT.pkl', 'wb') as f:\n",
    "    pickle.dump(history_gat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [00:01<00:00, 861.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.nn import GlobalAttentionPooling\n",
    "\n",
    "# initialize the gate layer\n",
    "gate_nn = torch.nn.Linear(512, 1)\n",
    "# initialize the GlobalAttentionPooling layer\n",
    "gap = GlobalAttentionPooling(gate_nn)\n",
    "\n",
    "'''For each history graph, we need to convert it to a DGL graph and then apply the GlobalAttentionPooling layer'''\n",
    "history_GAP = []\n",
    "\n",
    "for i in tqdm(range(len(history_gat))):\n",
    "    # initialize an empty dgl graph containing 11 nodes\n",
    "    g = dgl.graph(([], []), num_nodes=history_gat[i].num_nodes)\n",
    "    # initialize the node features for the dgl graph where each node has 512 features\n",
    "    g_node_feats = history_gat[i].x\n",
    "    # add the edges to the dgl graph where edges are the edges from the pytorch geometric graph\n",
    "    g.add_edges(history_gat[i].edge_index[0], history_gat[i].edge_index[1])\n",
    "    # apply the GlobalAttentionPooling layer to the dgl graph\n",
    "    out = gap(g, g_node_feats)\n",
    "    # append the output to the history_GAP list\n",
    "    history_GAP.append(out.squeeze())\n",
    "\n",
    "'''Save the History Graph Attention Pooling List to a pickle file in the proper directory'''\n",
    "with open('../embeddings/history/'+ str(subset) +'/history_GAP.pkl', 'wb') as f:\n",
    "    pickle.dump(history_GAP, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create a DataLoader object to load the Data objects in batches'''\n",
    "history_loader = DataLoader(history_gat, batch_size=32, shuffle=False)\n",
    "\n",
    "with open('../embeddings/history/' + str(subset) + '/history_loader.pkl', 'wb') as f:\n",
    "    pickle.dump(history_loader, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Also implement the case where there's an edge_attr which is the coreference score'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bigpool/homes/chatterjee/anaconda3/envs/e2ecoref/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /bigpool/homes/chatterjee/anaconda3/envs/e2ecoref/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/bigpool/homes/chatterjee/anaconda3/envs/e2ecoref/lib/python3.7/site-packages/rich/live.py:229: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/bigpool/homes/chatterjee/anaconda3/envs/e2ecoref/lib/python3.7/site-packages/rich/live.py:229: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ee1629ab28482bb9dcfd7fdedb81b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 414/414 [00:00<00:00, 163kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 506kB/s]  \n",
      "Downloading: 100%|██████████| 634M/634M [00:12<00:00, 53.8MB/s] \n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''AllenNLP Coreference Resolution'''\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "'''download the model'''\n",
    "predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''AllenNLP Coreference Resolution:\n",
    "    Method: Input two sentences and return true or false if they have a coreference using the AllenNLP coreference model'''\n",
    "\n",
    "def coref(text1, text2):\n",
    "    text = text1 + ' ' + text2\n",
    "    prediction = predictor.predict(document=text)\n",
    "    # find corresponding strings from the text and create a cluster entity list\n",
    "    entity_clusters = []\n",
    "    for entity in prediction['clusters']:\n",
    "        entity_cluster = []\n",
    "        for entry in entity:\n",
    "            entity_cluster.append(' '.join(text.split()[entry[0]:entry[1]+1]))\n",
    "        # remove empty strings\n",
    "        entity_cluster = [entry for entry in entity_cluster if entry != '']\n",
    "        entity_clusters.append(entity_cluster)\n",
    "\n",
    "    # if all the entities in a cluster are in the same sentence, then return False (no coreference)\n",
    "    for entity_cluster in entity_clusters:\n",
    "        which_sentence = []\n",
    "        for entity in entity_cluster:\n",
    "            if entity in text1 and entity not in text2:\n",
    "                which_sentence.append(1)\n",
    "            elif entity in text2 and entity not in text1:\n",
    "                which_sentence.append(2)\n",
    "        if len(set(which_sentence)) == 1:\n",
    "            return False\n",
    "    # if all the entities in a cluster are in different sentences, then return True (coreference)\n",
    "    return True\n",
    "\n",
    "\n",
    "coref('This is a picture of a horse and a man standing by a stack of crates.',\n",
    "      'Is the horse brown? Yes, it is.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implementation of the e2e-coref model'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnnVD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
