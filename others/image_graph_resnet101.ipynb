{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "# load the subset of the dataset\n",
    "data = json.load(open('../data/subsets/visdial_1.0_train_10percent_subset.json'))['data']\n",
    "dialogs = data['dialogs']\n",
    "image_paths = [index['image_id'] for index in data['dialogs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_final_a3ec72.pkl: 254MB [00:12, 20.7MB/s]                           \n",
      "/bigpool/homes/chatterjee/anaconda3/envs/gnnVD/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 12329/12329 [3:50:54<00:00,  1.12s/it] \n"
     ]
    }
   ],
   "source": [
    "'''Instance segmentation for all images in the subset dataset\n",
    "Mask R-CNN is used in this case with ResNet-50-FPN backbone'''\n",
    "# import some common detectron2 utilities, some image processing libraries, and setup detectron2 logger\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "%matplotlib inline\n",
    "\n",
    "'''Takes in pred_boxes Box tensors and converts them to a list of integer [x, y, x_width, y_height] (where the order of the list is maintained while creation) for cropping the image'''\n",
    "def int_box_coordinates(pred_boxes):\n",
    "    box_coordinates = []\n",
    "    for pred_box in pred_boxes:\n",
    "        # x,y is the top left corner of the box and x_width, y_height is the bottom right corner of the box\n",
    "        # print(pred_box.cpu().numpy()[0])\n",
    "        box_coordinates.append([int(pred_box.cpu().numpy()[0]),\n",
    "                                int(pred_box.cpu().numpy()[1]),\n",
    "                                int(pred_box.cpu().numpy()[2]),\n",
    "                                int(pred_box.cpu().numpy()[3])])\n",
    "    return box_coordinates\n",
    "\n",
    "\n",
    "'''Takes in an image path and returns a list of [y:y_height, x:x_width] based on all the image segments from a detectron2 model'''\n",
    "def instance_segmentation(image_path):\n",
    "    # load the image from the path provided\n",
    "    img = cv2.imread(image_path)\n",
    "    # create a configuration for the model\n",
    "    cfg = get_cfg()\n",
    "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\n",
    "        \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "        \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "    # cfg.MODEL.DEVICE = \"cpu\"  # because I don't have a local GPU\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    outputs = predictor(img)\n",
    "    # look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "    pred_boxes = outputs[\"instances\"].pred_boxes\n",
    "    return int_box_coordinates(pred_boxes)\n",
    "\n",
    "# run the instance segmentation on the images\n",
    "instance_boxes = [instance_segmentation(image_path) for image_path in tqdm(image_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the instance segments on the subset_data\n",
    "with open('../embeddings/images/instance/10/res101_bounding_boxes.json', 'w') as outfile:\n",
    "    json.dump(instance_boxes, outfile)\n",
    "\n",
    "# note that the order of the coordinates are x, y, x_width, y_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "# load the instance segments on the subset_data\n",
    "instance_boxes = json.load(open('../embeddings/images/instance/10/res101_bounding_boxes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://codereview.stackexchange.com/questions/31352/overlapping-rectangles\n",
    "class Point(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Rect(object):\n",
    "    def __init__(self, p1, p2):\n",
    "        '''Store the top, bottom, left and right values for points \n",
    "               p1 and p2 are the (corners) in either order\n",
    "        '''\n",
    "        self.left = min(p1.x, p2.x)\n",
    "        self.right = max(p1.x, p2.x)\n",
    "        self.bottom = min(p1.y, p2.y)\n",
    "        self.top = max(p1.y, p2.y)\n",
    "\n",
    "    @staticmethod\n",
    "    def overlap(r1, r2):\n",
    "        '''Overlapping rectangles overlap both horizontally & vertically\n",
    "        '''\n",
    "        h_overlaps = (r1.left <= r2.right) and (r1.right >= r2.left)\n",
    "        v_overlaps = (r1.bottom <= r2.top) and (r1.top >= r2.bottom)\n",
    "        return h_overlaps and v_overlaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in instance_boxes].count(0)\n",
    "# seems like there's 10 items with no instance segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67,\n",
       " 107,\n",
       " 161,\n",
       " 276,\n",
       " 350,\n",
       " 374,\n",
       " 450,\n",
       " 584,\n",
       " 610,\n",
       " 766,\n",
       " 885,\n",
       " 1550,\n",
       " 1573,\n",
       " 1626,\n",
       " 1978,\n",
       " 2629,\n",
       " 2632,\n",
       " 2675,\n",
       " 2779,\n",
       " 2799,\n",
       " 3049,\n",
       " 3166,\n",
       " 3308,\n",
       " 3454,\n",
       " 3612,\n",
       " 3682,\n",
       " 3717,\n",
       " 3867,\n",
       " 3964,\n",
       " 4343,\n",
       " 4381,\n",
       " 4539,\n",
       " 4631,\n",
       " 4920,\n",
       " 5177,\n",
       " 5303,\n",
       " 5580,\n",
       " 6047,\n",
       " 6121,\n",
       " 6399,\n",
       " 6423,\n",
       " 6490,\n",
       " 6587,\n",
       " 6660,\n",
       " 6709,\n",
       " 6935,\n",
       " 7006,\n",
       " 7012,\n",
       " 7127,\n",
       " 7163,\n",
       " 7464,\n",
       " 7644,\n",
       " 7991,\n",
       " 8096,\n",
       " 8111,\n",
       " 8181,\n",
       " 8204,\n",
       " 8209,\n",
       " 8401,\n",
       " 8541,\n",
       " 8725,\n",
       " 8743,\n",
       " 8843,\n",
       " 8959,\n",
       " 9029,\n",
       " 9037,\n",
       " 9069,\n",
       " 9105,\n",
       " 9187,\n",
       " 9542,\n",
       " 9579,\n",
       " 10168,\n",
       " 10430,\n",
       " 10753,\n",
       " 11000,\n",
       " 11245,\n",
       " 11305,\n",
       " 11701,\n",
       " 11833,\n",
       " 12039,\n",
       " 12111,\n",
       " 12127]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices of the items with no instance segments\n",
    "list(np.where(np.array([len(i) for i in instance_boxes]) == 0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12329/12329 [00:03<00:00, 3871.42it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_list = []\n",
    "for image_instances in tqdm(instance_boxes):  # n segments for each image\n",
    "    interactions = []\n",
    "    # apparently there are some images with no instance segments\n",
    "    if len(image_instances) != 0:\n",
    "        # initialize with the first interaction as a list\n",
    "        interactions_list = [image_instances[0]]\n",
    "    for instance in image_instances[1:]:  # n-1 interactions for each image\n",
    "        for each_interaction in interactions_list:\n",
    "            # check if the current interaction overlaps with any of the previous interactions\n",
    "            # note that the order of the coordinates from the list are y:y_height, x:x_width\n",
    "            l1 = Point(instance[2], instance[0])\n",
    "            r1 = Point(instance[3], instance[1])\n",
    "            l2 = Point(each_interaction[2], each_interaction[0])\n",
    "            r2 = Point(each_interaction[3], each_interaction[1])\n",
    "            # if it does, add an edge between the two where edge index is based on its position in the image_instances list\n",
    "            if Rect.overlap(Rect(l1, r1), Rect(l2, r2)):\n",
    "                interactions.append([image_instances.index(\n",
    "                    each_interaction), image_instances.index(instance)])\n",
    "            # print(\"Checking between\", [image_instances.index(each_interaction), image_instances.index(instance)])\n",
    "        # append the current interaction to the interactions_list\n",
    "        interactions_list.append(instance)\n",
    "    edge_list.append(interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the instance segments on the subset_data\n",
    "with open('../embeddings/images/instance/10/res101_edge_list.json', 'w') as outfile:\n",
    "    json.dump(edge_list, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = []\n",
    "for element in edge_list:\n",
    "    # size here is\n",
    "    size = len(instance_boxes[edge_list.index(element)])\n",
    "    # initialize adjacency matrix\n",
    "    adj_matrix_element = [[0 for i in range(size)] for j in range(size)]\n",
    "    # add edges to the adjacency matrix\n",
    "    if len(element) != 0:\n",
    "        for row, col in element:\n",
    "            adj_matrix_element[row][col] = 1\n",
    "    # add the adjacency matrix to the list of adjacency matrices\n",
    "    adj_matrix.append(adj_matrix_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the instance segments on the subset_data\n",
    "with open('../embeddings/images/instance/10/res101_adj_list.json', 'w') as outfile:\n",
    "    json.dump(adj_matrix, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnnVD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
