One of the main challenges in Computer Vision and Natural Language Processing is enabling machines to have a conversation with humans about visual data and to understand and interpret the meaning and context in a conversational dialog. Towards this, researchers have proposed the Visual Dialog Challenge dataset which aims to create a basis on which multi-modal language and vision models can be trained and fine-tuned to reach this milestone. One promising approach apart from pre-trained language and vision models is the use of graph neural networks to model the different implicit relations between objects in an image and dialog. However, they often neglect the importance of coreference relations in dialog and the representation of an image under the context of full-question comprehension based on past dialogs. To improve this Chen et al. (2021) propose a novel architecture that uses three sequential graphs to explicitly model relations between the dialog, question, and image which this thesis replicates. The GoG approach is also shown to outperform strong baseline implementations for the task. However, the explicit nature of relation modeling might not be the best approach to perform relation modeling. To address this, this thesis proposes an implicit relation modeling between the three sequential graphs using a master node using graph attention. Experimental results indicate a good starting point, however, it also advocates the need for further experimentation to confirm the validity of this novel architecture.

Reference:
Feilong Chen, Xiuyi Chen, Fandong Meng, Peng Li, and Jie Zhou. Gog: Relation-aware graph-over-graph network for visual dialog. CoRR, abs/2109.08475, 2021
