{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Since all the Dependency Heads, the Dependency Labels, and the Linearized Constituency Trees are generated in batches of 10000,\n",
    "we need to concatenate those files.'''\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "def concatenate_files(target_files, output_file):\n",
    "    with open(output_file, 'wb') as outfile:\n",
    "        # natsorted is used to sort the files in a natural way\n",
    "        for filename in natsorted(glob.glob(target_files)):\n",
    "            if filename == output_file:\n",
    "                # don't want to copy the output into the output\n",
    "                continue\n",
    "            with open(filename, 'rb') as read_file:\n",
    "                shutil.copyfileobj(read_file, outfile)\n",
    "\n",
    "# set the data subset to use\n",
    "subset = 1\n",
    "\n",
    "# for the dependency heads\n",
    "target_files = '../embeddings/questions/' + str(subset) + '/dependency_heads/*.txt'\n",
    "output_file = '../embeddings/questions/' + str(subset) + '/dependency_heads.txt'\n",
    "concatenate_files(target_files, output_file)\n",
    "\n",
    "# for the dependency labels\n",
    "target_files = '../embeddings/questions/' + str(subset) + '/dependency_labels/*.txt'\n",
    "output_file = '../embeddings/questions/' + str(subset) + '/dependency_labels.txt'\n",
    "concatenate_files(target_files, output_file)\n",
    "\n",
    "# for the linearized constituency trees\n",
    "target_files = '../embeddings/questions/' + str(subset) + '/linearized_constituency_tree/*.txt'\n",
    "output_file = '../embeddings/questions/' + str(subset) + '/linearized_constituency_tree.txt'\n",
    "concatenate_files(target_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is bananas next to glass?',\n",
       " 'is he wearing all white?',\n",
       " 'what does pet bed look like?',\n",
       " 'how old is the person holding the kite?',\n",
       " 'what is the age of the men?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here we read the Questions'''\n",
    "# set the data subset to use\n",
    "subset = 1\n",
    "\n",
    "with open('../embeddings/questions/' + str(subset) + '/questions_corrected.txt', 'r') as f:\n",
    "    questions = f.readlines()\n",
    "    questions = [x.strip() for x in questions] \n",
    "    # removes the newlines and the questions since apparently LAL parser doesn't work on all the questions with ? at the end\n",
    "    # only applicable for the 100% dataset\n",
    "    # questions = [x.strip('?\\n') for x in questions] \n",
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 3, 4, 1],\n",
       " [3, 3, 0, 5, 3, 3],\n",
       " [5, 5, 4, 5, 0, 5, 5],\n",
       " [2, 3, 0, 5, 3, 5, 8, 6, 3],\n",
       " [4, 4, 4, 0, 4, 7, 5, 4]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here we read the Dependency Heads'''\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "with open('../embeddings/questions/' + str(subset) + '/dependency_heads_corrected.txt', 'r') as f:\n",
    "    dep_head_list = f.readlines()\n",
    "    dep_head_list = [element.strip() for element in dep_head_list]\n",
    "    dep_head_list = [ast.literal_eval(element) for element in dep_head_list]\n",
    "\n",
    "dep_head_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['root', 'nsubj', 'advmod', 'prep', 'pobj', 'punct'],\n",
       " ['aux', 'nsubj', 'root', 'det', 'dobj', 'punct'],\n",
       " ['pobj', 'aux', 'dep', 'nsubj', 'root', 'prep', 'punct'],\n",
       " ['advmod', 'dep', 'root', 'det', 'nsubj', 'partmod', 'det', 'dobj', 'punct'],\n",
       " ['nsubj', 'cop', 'det', 'root', 'prep', 'det', 'pobj', 'punct']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Similarly, we read the Dependency Labels'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/dependency_labels_corrected.txt', 'r') as f:\n",
    "    dep_label_list = f.readlines()\n",
    "    dep_label_list = [element.strip() for element in dep_label_list]\n",
    "    dep_label_list = [ast.literal_eval(element) for element in dep_label_list]\n",
    "\n",
    "'''Note: Here we probably have to ignore the last label of each sentence, since it is always a punctuation mark and the LAL-Parser doesn't tag relations between punctuation marks.'''\n",
    "dep_label_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([11, 1, 2, 3, 4, 11, 11, 11, 11, 11, 0, 11],\n",
       " ['partmod',\n",
       "  'prep',\n",
       "  'pobj',\n",
       "  'prep',\n",
       "  'pobj',\n",
       "  'punct',\n",
       "  'cop',\n",
       "  'nsubj',\n",
       "  'det',\n",
       "  'nn',\n",
       "  'root',\n",
       "  'punct'],\n",
       " 'parked in front of hydrant, is there an emergency scene?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was a problem sentence\n",
    "dep_head_list[912], dep_label_list[912], questions[912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 1, 3, 4, 1],\n",
       " ['root', 'nsubj', 'advmod', 'prep', 'pobj', 'punct'],\n",
       " 'is banana next to glass?')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_head_list[0], dep_label_list[0], questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8402/8402 [00:00<00:00, 104733.26it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Formulate the adjacency list based on the dependency heads and save it to a file'''\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "adj_list = []\n",
    "for element in tqdm(dep_head_list):\n",
    "    adj_list_element = []\n",
    "    for index in element:\n",
    "        adj_list_node = len(element)*[0]\n",
    "        # print(index, adj_list_node, len(element))\n",
    "        '''the index-1 < len(element) needs to be added for the 100% dataset since the the tagging of the ? to the root node is done after the LAL-Parser parsed questions and generated the dependency heads and labels without the questions having a '?' at the end. This is then solved by the stanford parser and the stanford parser parses sentences like 'is it a full-grown elephant?' (example 912) with the root of the sentence being at index 7 which to the LAL-Parser method is out of bounds.''' \n",
    "        if index != 0 and index-1 < len(element):\n",
    "            adj_list_node[index-1] = 1 \n",
    "        adj_list_element.append(adj_list_node)\n",
    "\n",
    "    adj_list.append(adj_list_element)\n",
    "\n",
    "# convert the directed adjacency list to an undirected adjacency list by adding the transpose of the adjacency matrix to the adjacency matrix\n",
    "adj_list = [np.array(element) + np.array(element).T for element in adj_list]\n",
    "# for each adjacency matrix, \n",
    "sym_adj_list = []\n",
    "for a_list in adj_list:\n",
    "    # if elements in a_list are 2, set them to 1\n",
    "    a_list[a_list == 2] = 1\n",
    "    # add the adjacency matrix to the list\n",
    "    sym_adj_list.append(a_list.tolist())\n",
    "\n",
    "with open('../embeddings/questions/' + str(subset) + '/adj_list_corrected.json', 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    # if the data is nested\n",
    "    json.dump(sym_adj_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "'''Displaying the first adjacency matrix'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/adj_list_corrected.json', 'r') as f:\n",
    "    adj_list = json.load(f)\n",
    "\n",
    "adj_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Also formulate the edge list if needed based on the dependency heads'''\n",
    "edge_list = []\n",
    "for element in dep_head_list:\n",
    "    edge_list_element = []\n",
    "    for index in range(len(element)):\n",
    "        if element[index] != 0:\n",
    "            # symmetrize the edge list\n",
    "            edge_list_element.append([index, element[index]-1])\n",
    "            edge_list_element.append([element[index]-1, index])\n",
    "    edge_list.append(edge_list_element)\n",
    "\n",
    "with open('../embeddings/questions/' + str(subset) + '/edge_list_corrected.json', 'w') as f:\n",
    "    # indent=2 is not needed but makes the file human-readable\n",
    "    # if the data is nested\n",
    "    json.dump(edge_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 1], [2, 0], [0, 2], [3, 2], [2, 3], [4, 3], [3, 4], [5, 0], [0, 5]]\n"
     ]
    }
   ],
   "source": [
    "'''Displaying the first edge list'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/edge_list_corrected.json', 'r') as f:\n",
    "    edge_list = json.load(f)\n",
    "\n",
    "print(edge_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove-6b-300d Model:\n",
      "Done. Glove-6b-300d with a vocabulary of 399998 words was loaded!\n"
     ]
    }
   ],
   "source": [
    "'''Loading Glove'''\n",
    "import pandas as pd\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    readable_name = \"-\".join(glove_file.rstrip('.txt').split('/')[-1].split(\".\")).capitalize()\n",
    "    print(f\"Loading {readable_name} Model:\")\n",
    "    df = pd.read_csv(glove_file, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    glove_model = {key: val.values for key, val in df.T.items()}\n",
    "    print(f\"Done. {readable_name} with a vocabulary of {len(glove_model)} words was loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_6b_300 = load_glove_model('../embeddings/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_514784/14260044.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.tensor(hidden_states)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Tokenize a sentence using a glove model, pass it through an LSTM and return the hidden state of each word'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_hidden_state(sentence, glove_model, hidden_dim, batch_size):\n",
    "    # tokenize the sentence: words, punctuations are individual tokens\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    # get the glove vectors for each word\n",
    "    glove_vectors = []\n",
    "    for word in tokenized_sentence:\n",
    "        if word in glove_model:\n",
    "            glove_vectors.append(glove_model[word])\n",
    "        else:\n",
    "            glove_vectors.append(glove_model['unk'])\n",
    "    # create the input tensor\n",
    "    input_tensor = torch.FloatTensor(np.array(glove_vectors))\n",
    "    # create the LSTM\n",
    "    lstm = nn.LSTM(300, hidden_dim)\n",
    "    # take the fractal sequence of the input tensor, i.e. first word, first and second word, first, second and third word, etc., pass it through the LSTM and save the hidden state of each word\n",
    "    hidden_states = []\n",
    "    for i in range(len(tokenized_sentence)):\n",
    "        hidden_state, _ = lstm(input_tensor[:i+1].unsqueeze(1))\n",
    "        hidden_states.append(hidden_state[-1].detach().numpy().squeeze())\n",
    "    return torch.tensor(hidden_states)\n",
    "\n",
    "'''Test the function'''\n",
    "sentence = \"This is a test sentence?\"\n",
    "hidden_states = sentence_to_hidden_state(sentence, glove_6b_300, 300, 1)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmod', 'partmod', 'cop', 'quantmod', 'ccomp', 'csubj', 'iobj', 'advmod', 'appos', 'acomp', 'amod', 'dobj', 'auxpass', 'xcomp', 'prt', 'root', 'possessive', 'pobj', 'parataxis', 'discourse', 'infmod', 'pcomp', 'prep', 'dep', 'mark', 'mwe', 'punct', 'cc', 'nsubjpass', 'rcmod', 'advcl', 'expl', 'neg', 'preconj', 'predet', 'det', 'aux', 'nn', 'number', 'conj', 'nsubj', 'csubjpass', 'npadvmod', 'num', 'poss']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import itertools\n",
    "\n",
    "'''Finds the unique dependency labels generated by the LAL-parser'''\n",
    "with open('../embeddings/questions/' + str(100) + '/dependency_labels.txt', 'r') as f:\n",
    "    LAL_Parser_Labels = f.readlines()\n",
    "    LAL_Parser_Labels = [element.strip() for element in LAL_Parser_Labels]\n",
    "    LAL_Parser_Labels = [ast.literal_eval(element) for element in LAL_Parser_Labels]\n",
    "\n",
    "unique_labels = list(dict.fromkeys(\n",
    "    set(list(itertools.chain(*LAL_Parser_Labels)))))\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8402/8402 [00:00<00:00, 133014.30it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Assigns a unique integer to each dependency label based on the label's position in the unique_labels list'''\n",
    "\n",
    "label_to_int = []\n",
    "\n",
    "for dep_labels in tqdm(dep_label_list):\n",
    "    label_to_int.append([unique_labels.index(label) for label in dep_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8402/8402 [01:10<00:00, 119.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# save the node embeddings for all questions\n",
    "LSTM_node_embeddings = [sentence_to_hidden_state(question, glove_6b_300, 300, 1) for question in tqdm(questions)]\n",
    "\n",
    "'''Make all the LSTM node embeddings in the shape of 20x300 by padding with zeros'''\n",
    "\n",
    "LSTM_node_embeddings_padded = []\n",
    "for node_embedding in LSTM_node_embeddings:\n",
    "    if node_embedding.shape[0] < 20:\n",
    "        LSTM_node_embeddings_padded.append(\n",
    "            np.pad(node_embedding, ((0, 20-node_embedding.shape[0]), (0, 0)), 'constant'))\n",
    "    else:\n",
    "        LSTM_node_embeddings_padded.append(node_embedding[:20])\n",
    "\n",
    "'''Save the node embeddings in a pickle file'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/LSTM_node_embeddings_corrected.pickle', 'wb') as f:\n",
    "    pickle.dump(LSTM_node_embeddings_padded, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "'''Load the node embeddings from the pickle file'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/LSTM_node_embeddings_corrected.pickle', 'rb') as f:\n",
    "    LSTM_node_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_node_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each item in the edge list, if a node is less than 20, keep it, otherwise, discard it\n",
    "edge_list_padded = []\n",
    "for edge_list_entry in edge_list:\n",
    "    edge_list_padded_entry = []\n",
    "    for edge in edge_list_entry:\n",
    "        if edge[0] < 20 and edge[1] < 20:\n",
    "            edge_list_padded_entry.append(edge)\n",
    "    edge_list_padded.append(edge_list_padded_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each entry in the label_to_int list, if value is less than 20, keep it. Discard the rest\n",
    "label_to_int_padded = []\n",
    "for label in label_to_int:\n",
    "    if len(label) < 20:\n",
    "        label_to_int_padded.append(label)\n",
    "    else:\n",
    "        label_to_int_padded.append(label[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8402/8402 [00:00<00:00, 29360.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''Create a list of Data objects for each question where:\n",
    "    - x is the LSTM_node_embeddings\n",
    "    - edge_index is the edge list where they are contiguous\n",
    "    - edge_attr is label_to_int'''\n",
    "question_graphs = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    question_graphs.append(Data(x=torch.FloatTensor(LSTM_node_embeddings[i]), \n",
    "                                edge_index=torch.LongTensor(\n",
    "                                    edge_list_padded[i]).t().contiguous(),\n",
    "                                edge_attr=torch.LongTensor(label_to_int_padded[i])))\n",
    "\n",
    "'''Save the Data objects to a file'''\n",
    "import pickle\n",
    "\n",
    "with open('../embeddings/questions/' + str(subset) + '/question_graphs_corrected.pkl', 'wb') as f:\n",
    "    pickle.dump(question_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrView(store=Data(x=[20, 300], edge_index=[2, 10], edge_attr=[6]), attr=DataTensorAttr(group_name=None, attr_name=<FieldStatus.UNSET: 1>, index=<FieldStatus.UNSET: 1>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the Data objects from the file'''\n",
    "import pickle\n",
    "\n",
    "# set the subset number\n",
    "subset = 1\n",
    "\n",
    "with open('../embeddings/questions/' + str(subset) + '/question_graphs_corrected.pkl', 'rb') as f:\n",
    "    question_graphs = pickle.load(f)\n",
    "\n",
    "question_graphs[0].view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Visualize the graph'''\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def visualize_graph(graph):\n",
    "    G = to_networkx(graph)\n",
    "    # get a list of nodes in G where the corresponding node in the data object node is zero\n",
    "    zero_nodes = [node for node in G.nodes if graph.x[node].sum() == 0]\n",
    "    # remove the zero nodes from G\n",
    "    G.remove_nodes_from(zero_nodes)\n",
    "    # initialize the figure and axes | Note: This somehow gets around the \"TypeError: '_AxesStack' object is not callable\"\n",
    "    fig, ax = plt.subplots() \n",
    "    # draw the graph\n",
    "    nx.draw(G, ax=ax, with_labels=True)\n",
    "    # set the figure size 4x4\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "visualize_graph(question_graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(\n",
    "            num_heads, 2 * c_out))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "        node_feats_flat = node_feats.view(\n",
    "            batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat,\n",
    "                               index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat,\n",
    "                               index=edge_indices_col, dim=0)\n",
    "        ], dim=-1)  # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(\n",
    "            adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(\n",
    "            1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For each question graph, convert the edge list into an adjacency matrix which now captures the updated padded node sizes.'''\n",
    "adj_list_padded = []\n",
    "\n",
    "for graph in question_graphs:\n",
    "    # Get the edge list and transpose it\n",
    "    edge_list = graph.edge_index.t()\n",
    "    # Get the number of nodes in the graph\n",
    "    num_nodes = graph.num_nodes\n",
    "    # initialize the adjacency matrix for the graph\n",
    "    adj_matrix_graph = [[0 for i in range(num_nodes)]\n",
    "                        for j in range(num_nodes)]\n",
    "    if len(edge_list) != 0:\n",
    "        for row, col in edge_list:\n",
    "            # if row and col are greater than 20, the we ignore them\n",
    "            if row < 20 and col < 20:\n",
    "                adj_matrix_graph[row][col] = 1\n",
    "    adj_list_padded.append(adj_matrix_graph)\n",
    "\n",
    "'''Save the adjacency matrices in a json file'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/adj_list_padded_corrected.json', 'w') as f:\n",
    "    json.dump(adj_list_padded, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 300])\n",
      "torch.Size([20, 512])\n"
     ]
    }
   ],
   "source": [
    "'''Initialize the GAT layer with:\n",
    "- input dimension of 300 (the dimensionality of the node features)\n",
    "- output dimension of 512 (the dimensionality of the output features)\n",
    "- 4 attention heads\n",
    "- attention heads are not concatenated\n",
    "- alpha is set to 0.2\n",
    "Note: The features here are set from the GoG Paper'''\n",
    "\n",
    "gat_layer = GATLayer(c_in=300, c_out=512, num_heads=4, concat_heads=False, alpha=0.2)\n",
    "\n",
    "# testing the GAT layer with the first graph\n",
    "with torch.no_grad():\n",
    "    out_feats = gat_layer(node_feats=question_graphs[0].x.unsqueeze(0), adj_matrix= torch.tensor([adj_list_padded[0]]), print_attn_probs=False)\n",
    "\n",
    "print(question_graphs[0].x.squeeze().shape)\n",
    "print(out_feats.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8402/8402 [00:05<00:00, 1539.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "'''Do this for all Question Graphs and save it to the Question Graph Attention List'''\n",
    "question_gat = question_graphs.copy()\n",
    "for i in tqdm(range(len(question_graphs))):\n",
    "    with torch.no_grad():\n",
    "        out_feats = gat_layer(node_feats=question_graphs[i].x.unsqueeze(0), adj_matrix= torch.tensor([adj_list_padded[i]]), print_attn_probs=False)\n",
    "    question_gat[i].x = out_feats.squeeze()\n",
    "\n",
    "'''Save the Question Graph Attention List to a pickle file'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/question_gat_corrected.pkl', 'wb') as f:\n",
    "    pickle.dump(question_gat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8402/8402 [00:08<00:00, 951.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.nn import GlobalAttentionPooling\n",
    "\n",
    "# initialize the gate layer\n",
    "gate_nn = torch.nn.Linear(512, 1)\n",
    "# initialize the GlobalAttentionPooling layer\n",
    "gap = GlobalAttentionPooling(gate_nn)\n",
    "\n",
    "'''For each question graph, we need to convert it to a DGL graph and then apply the GlobalAttentionPooling layer'''\n",
    "question_GAP = []\n",
    "\n",
    "for i in tqdm(range(len(question_gat))):\n",
    "    # initialize an empty dgl graph containing lambda nodes\n",
    "    g = dgl.graph(([], []), num_nodes=question_gat[i].num_nodes)\n",
    "    # initialize the node features for the dgl graph where each node has 512 features\n",
    "    g_node_feats = question_gat[i].x\n",
    "    # add the edges to the dgl graph where edges are the edges from the pytorch geometric graph\n",
    "    g.add_edges(question_gat[i].edge_index[0], question_gat[i].edge_index[1])\n",
    "    # apply the GlobalAttentionPooling layer to the dgl graph\n",
    "    out = gap(g, g_node_feats)\n",
    "    # append the output to the history_GAP list\n",
    "    question_GAP.append(out.squeeze())\n",
    "\n",
    "'''Save the Question Graph Attention Pooling List to a pickle file in the proper directory'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/question_GAP_corrected.pkl', 'wb') as f:\n",
    "    pickle.dump(question_GAP, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnnVD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
