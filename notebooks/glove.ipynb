{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove-6b-300d Model:\n",
      "Done. Glove-6b-300d with a vocabulary of 399998 words was loaded!\n",
      "Loading Glove-twitter-27b-200d Model:\n",
      "Done. Glove-twitter-27b-200d with a vocabulary of 1193513 words was loaded!\n",
      "Loading Glove-840b-300d Model:\n",
      "Done. Glove-840b-300d with a vocabulary of 2196009 words was loaded!\n"
     ]
    }
   ],
   "source": [
    "'''Load the three main flavours of GloVe models'''\n",
    "import pandas as pd\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    readable_name = \"-\".join(glove_file.rstrip('.txt').split('/')[-1].split(\".\")).capitalize()\n",
    "    print(f\"Loading {readable_name} Model:\")\n",
    "    df = pd.read_csv(glove_file, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    glove_model = {key: val.values for key, val in df.T.items()}\n",
    "    print(f\"Done. {readable_name} with a vocabulary of {len(glove_model)} words was loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_6b_300 = load_glove_model('../embeddings/glove/glove.6B.300d.txt')\n",
    "glove_twitter_27b_200 = load_glove_model('../embeddings/glove/glove.twitter.27B.200d.txt')\n",
    "glove_840b_300 = load_glove_model('../embeddings/glove/glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocab: 15697\n",
      "Number of words in the vocab not present in glove.6B.300d.txt: 487\n",
      "Number of words in the vocab not present in glove.twitter.27B.200d.txt: 805\n",
      "Number of words in the vocab not present in glove.840B.300d.txt: 161\n"
     ]
    }
   ],
   "source": [
    "'''Checking the Out-of-Vocabulary (OOV) words for the Questions'''\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# read text file\n",
    "with open('../data/visdial_1.0_train_questions.txt') as file:\n",
    "    questions = [line.rstrip() for line in file]\n",
    "\n",
    "# remove punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  # to remove punctuations\n",
    "tokenized_questions = [tokenizer.tokenize(question.lower()) for question in questions]\n",
    "\n",
    "# checking if the words in the vocab are present in the glove model\n",
    "question_vocab = set(list(itertools.chain.from_iterable(tokenized_questions)))\n",
    "q_oov_glove_6b_300 = [word for word in question_vocab if word not in glove_6b_300.keys()]\n",
    "q_oov_glove_twitter_27b_200 = [word for word in question_vocab if word not in glove_twitter_27b_200.keys()]\n",
    "q_oov_glove_840b_300 = [word for word in question_vocab if word not in glove_840b_300.keys()]\n",
    "\n",
    "print(f\"Number of words in the vocab: {len(question_vocab)}\")\n",
    "print(f\"Number of words in the vocab not present in glove.6B.300d.txt: {len(q_oov_glove_6b_300)}\")\n",
    "print(f\"Number of words in the vocab not present in glove.twitter.27B.200d.txt: {len(q_oov_glove_twitter_27b_200)}\")\n",
    "print(f\"Number of words in the vocab not present in glove.840B.300d.txt: {len(q_oov_glove_840b_300)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocab: 28280\n",
      "Number of words in the vocab not present in glove.6B.300d.txt: 3366\n",
      "Number of words in the vocab not present in glove.twitter.27B.200d.txt: 4336\n",
      "Number of words in the vocab not present in glove.840B.300d.txt: 1863\n"
     ]
    }
   ],
   "source": [
    "'''Checking the Out-of-Vocabulary (OOV) words for the History'''\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import itertools\n",
    "\n",
    "'''Load the history'''\n",
    "with open(\"../embeddings/history/100/history.json\", 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "# tokenize the history\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  # to remove punctuations\n",
    "history_flatlist = list(itertools.chain.from_iterable(history))\n",
    "tokenized_history = [tokenizer.tokenize(sentence.lower()) for sentence in history_flatlist]\n",
    "\n",
    "# checking if the words in the vocab are present in the glove model\n",
    "history_vocab = set(list(itertools.chain.from_iterable(tokenized_history)))\n",
    "h_oov_glove_6b_300 = [word for word in history_vocab if word not in glove_6b_300.keys()]\n",
    "h_oov_glove_twitter_27b_200 = [word for word in history_vocab if word not in glove_twitter_27b_200.keys()]\n",
    "h_oov_glove_840b_300 = [word for word in history_vocab if word not in glove_840b_300.keys()]\n",
    "\n",
    "print(f\"Number of words in the vocab: {len(history_vocab)}\")\n",
    "print(f\"Number of words in the vocab not present in glove.6B.300d.txt: {len(h_oov_glove_6b_300)}\")\n",
    "print(f\"Number of words in the vocab not present in glove.twitter.27B.200d.txt: {len(h_oov_glove_twitter_27b_200)}\")\n",
    "print(f\"Number of words in the vocab not present in glove.840B.300d.txt: {len(h_oov_glove_840b_300)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''Tokenize a sentence using a glove model, pass it through an LSTm and return the last LSTM hidden state'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sentence_LSTM(glove_model, sentence):\n",
    "    # tokenize the sentence\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # to remove punctuations\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence.lower())\n",
    "    \n",
    "    # get the glove embedding for each word in the sentence\n",
    "    glove_embeddings = []\n",
    "    for word in tokenized_sentence:\n",
    "        if word in glove_model.keys():\n",
    "            glove_embeddings.append(glove_model[word])\n",
    "        else:\n",
    "            glove_embeddings.append(glove_model['unk'])\n",
    "\n",
    "    # convert the glove embeddings to a tensor\n",
    "    glove_embeddings = torch.tensor(glove_embeddings).float()\n",
    "\n",
    "    # pass the glove embeddings through an LSTM\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "    lstm = nn.LSTM(input_size=300, hidden_size=300, num_layers=1, batch_first=True)\n",
    "    lstm_out, (h_n, c_n) = lstm(glove_embeddings.view(1, len(tokenized_sentence), 300))\n",
    "    \n",
    "    # return the last hidden state\n",
    "    return h_n.squeeze(0)\n",
    "\n",
    "'''Testing LSTM hidden state output'''\n",
    "sentence_LSTM(glove_6b_300, \"What is the color of the shirt?\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''e2e Coreference Resolution from the paper Higher-order Coreference Resolution with Coarse-to-fine Inference by Li et al. (2018)'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnnVD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
