{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {'footstool': 134, 'barricade': 40, 'tissue': 176, 'painter': 110, 'end': 3, 'commode': 84, 'footrest': 51, 'hallway': 293, 'fruit': 28, 'hangers': 188, 'weight': 243, 'vanity': 259, 'heater': 113, 'of': 98, 'pedestal': 69, ',': 306, 'ornament': 217, 'exercise': 262, 'piano': 250, 'treadmill': 124, 'purse': 197, 'bench': 160, 'landing': 242, 'shower': 102, 'hallways': 35, 'blanket': 282, 'soap': 96, 'lounge': 130, 'flower': 292, 'rod': 189, 'fence': 167, 'knickknack': 78, 'bike': 265, 'cushion': 155, 'drawers': 159, 'sink': 104, 'valence': 16, 'coat': 274, 'dispenser': 202, 'post': 162, 'handle': 87, 'statue': 161, 'refrigerator': 67, 'curtain': 233, 'sculpture': 122, 'seat': 106, 'game': 172, 'conditioner': 68, 'the': 228, 'up': 302, '60': 317, 'boxes': 196, 'turn': 297, 'vase': 173, 'dishwasher': 75, '2': 311, 'air': 86, 'toilet': 288, '90': 318, 'recreation': 246, 'drawer': 58, 'plants': 285, 'cloth': 213, 'mudroom': 180, 'locker': 7, 'plate': 115, 'find': 261, 'whiteboard': 108, 'stop': 305, 'machine': 266, 'setting': 147, 'saunas': 277, 'curtains': 283, 'coffee': 48, 'plant': 44, 'head': 175, 'blinds': 206, 'staircases': 33, '3': 312, 'ball': 59, 'fireplace': 136, 'kitchen': 289, 'tree': 46, 'bowl': 280, 'dryer': 181, 'bottle': 144, 'towel': 237, 'easy': 187, 'staircase': 93, 'foyer': 253, 'clothes': 53, 'cup': 210, 'headboard': 149, 'fan': 248, 'phone': 207, 'range': 72, 'degrees': 307, 'utensils': 18, 'pool': 119, 'trash': 92, 'shampoo': 66, 'screen': 55, 'mudrooms': 36, 'photo': 199, 'picture': 255, 'bottles': 252, '150': 320, 'meetingrooms': 276, 'room': 241, 'bed': 151, '5': 314, 'hanger': 260, 'bar': 30, 'exit': 200, 'flowers': 70, 'showcase': 286, 'bidet': 13, 'candles': 5, 'wooden': 80, 'nightstand': 194, 'spa': 251, 'fire': 232, 'rack': 42, 'vent': 198, '180': 321, 'speaker': 256, 'ottoman': 158, 'forward': 304, 'hose': 156, 'conference': 57, 'pan': 146, 'chest': 129, 'bedrooms': 215, 'case': 142, 'bedroom': 281, 'glass': 204, 'equipment': 219, 'wardrobe': 120, 'decorative': 10, 'chandelier': 291, 'dinner': 109, 'island': 54, 'clutter': 121, 'sign': 63, 'utility': 141, 'toy': 19, 'potted': 257, '30': 316, 'sofa': 26, 'spas': 140, 'guitar': 168, 'bedpost': 39, '6': 315, 'telephone': 91, 'shrubbery': 52, 'pillar': 62, 'shelving': 247, 'hat': 133, 'family': 152, 'wash': 21, 'rail': 17, 'scale': 236, 'bathroom': 77, 'massage': 22, 'faucet': 81, 'fencing': 295, 'decor': 166, 'teapot': 25, '4': 313, 'stove': 49, 'bucket': 79, 'fridge': 37, 'lobby': 88, 'bin': 117, 'shoes': 123, 'radiator': 201, 'chimney': 145, 'art': 174, 'breaker': 90, 'lampshade': 50, 'ladder': 226, 'entryways': 125, 'placemat': 268, 'by': 309, 'armchair': 229, 'painting': 245, 'pot': 74, 'gym': 220, 'basket': 126, 'swivel': 290, 'foyers': 76, 'bathtub': 231, 'bookcase': 138, 'table': 24, 'stand': 139, 'living': 61, 'bulletin': 186, 'bookshelf': 27, 'or': 103, 'bust': 223, 'plush': 14, 'candelabra': 34, 'globe': 279, 'lounges': 230, 'bathrooms': 273, 'keyboard': 163, 'sauna': 240, 'extinguisher': 137, 'balcony': 12, 'steps': 308, 'office': 85, 'books': 89, 'balconies': 64, 'entryway': 244, 'smoke': 190, 'panel': 100, 'computer': 118, 'shelf': 29, 'monitor': 270, 'can': 83, 'candle': 31, 'figure': 211, 'detector': 65, 'clock': 278, 'cooler': 95, 'in': 154, 'water': 254, 'tray': 227, 'sheet': 105, 'center': 150, 'brush': 275, 'lamp': 4, 'ridge': 205, 'casing': 184, 'alarm': 284, 'desk': 111, '120': 319, 'switch': 45, '1': 310, 'closet': 225, 'food': 239, 'offices': 249, 'place': 182, 'stool': 212, 'furniture': 165, 'candlestick': 47, 'hamper': 183, 'appliance': 203, 'laundry': 296, 'closets': 264, 'garages': 287, 'wire': 101, '<EOS>': 2, 'robe': 107, 'dress': 94, 'counter': 269, 'lobbies': 132, 'dresser': 192, '<UNK>': 1, 'weights': 71, 'box': 41, 'chair': 112, 'dining': 272, 'right': 301, 'microwave': 169, 'toilets': 38, 'book': 97, 'tap': 32, 'power': 271, 'car': 135, 'dish': 20, 'garage': 193, 'oven': 209, 'urn': 15, 'couch': 9, 'set': 218, 'toiletry': 43, 'pillow': 267, 'a': 179, 'go': 298, 'round': 216, 'classrooms': 143, 'paper': 6, 'hood': 235, 'basin': 99, 'printer': 214, 'control': 294, 'shelves': 258, 'left': 300, 'rooms': 56, 'maker': 23, 'suitcase': 222, 'container': 131, 'washing': 171, 'electric': 238, 'pillows': 116, 'railing': 208, 'look': 299, 'tv': 157, 'kitchens': 127, 'towels': 153, '.': 195, 'one': 221, 'down': 303, 'mask': 164, 'cupboard': 263, 'board': 60, 'storage': 73, 'an': 224, 'easel': 82, 'tool': 114, 'umbrella': 185, 'thermostat': 178, 'bag': 234, 'doll': 170, 'workout': 128, 'library': 177, 'mirror': 8, 'holder': 191, 'display': 148, 'trinket': 11, '<PAD>': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {word2id[word]:word for word in word2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "GLOVE_PATH = '/home/quang/Downloads/glove.840B.300d.txt'\n",
    "SAVE_PATH = '/home/quang/train_embedding_Glove_840_300d.pkl'\n",
    "\n",
    "\n",
    "# init weights\n",
    "embed_size = 300\n",
    "vocab_size = len(id2word)\n",
    "std = 1/np.sqrt(embed_size)\n",
    "weights = np.random.normal(0, scale=std, size=[vocab_size, embed_size])\n",
    "weights = weights.astype(np.float32)\n",
    "\n",
    "word_list = []\n",
    "\n",
    "# load weights from glove\n",
    "with open(GLOVE_PATH, encoding='utf-8', mode=\"r\") as textFile:\n",
    "    for line in textFile:\n",
    "        line = line.split()\n",
    "        word = ''.join(line[:-300])\n",
    "\n",
    "        idx = word2id.get(word, None)\n",
    "        \n",
    "        if idx is not None:\n",
    "            weights[idx] = np.array(line[-300:], dtype=np.float32)\n",
    "            word_list.append(word)\n",
    "\n",
    "# load state_dict\n",
    "torch_weights = torch.from_numpy(weights)\n",
    "embeddings = torch.nn.Embedding(len(word2id), 300)\n",
    "embeddings.weight.data = torch_weights\n",
    "\n",
    "# save\n",
    "torch.save(embeddings.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n",
      "utilityrooms\n",
      "<PAD>\n",
      "<EOS>\n",
      "toolrooms\n",
      "laundryrooms\n",
      "utilityroom\n",
      "familyrooms\n",
      "conferencerooms\n"
     ]
    }
   ],
   "source": [
    "for word in word2id:\n",
    "    if not word in word_list:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Vocabulary maintains a mapping between words and corresponding unique\n",
    "integers, holds special integers (tokens) for indicating start and end of\n",
    "sequence, and offers functionality to map out-of-vocabulary words to the\n",
    "corresponding token.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    A simple Vocabulary class which maintains a mapping between words and\n",
    "    integer tokens. Can be initialized either by word counts from the VisDial\n",
    "    v1.0 train dataset, or a pre-saved vocabulary mapping.\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_counts_path: str\n",
    "        Path to a json file containing counts of each word across captions,\n",
    "        questions and answers of the VisDial v1.0 train dataset.\n",
    "    min_count : int, optional (default=0)\n",
    "        When initializing the vocabulary from word counts, you can specify a\n",
    "        minimum count, and every token with a count less than this will be\n",
    "        excluded from vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "    SOS_TOKEN = \"<S>\"\n",
    "    EOS_TOKEN = \"</S>\"\n",
    "    UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "    PAD_INDEX = 0\n",
    "    SOS_INDEX = 1\n",
    "    EOS_INDEX = 2\n",
    "    UNK_INDEX = 3\n",
    "\n",
    "    def __init__(self, word_counts_path: str, min_count: int = 5):\n",
    "        if not os.path.exists(word_counts_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Word counts do not exist at {word_counts_path}\"\n",
    "            )\n",
    "\n",
    "        with open(word_counts_path, \"r\") as word_counts_file:\n",
    "            word_counts = json.load(word_counts_file)\n",
    "\n",
    "            # form a list of (word, count) tuples and apply min_count threshold\n",
    "            word_counts = [\n",
    "                (word, count)\n",
    "                for word, count in word_counts.items()\n",
    "                if count >= min_count\n",
    "            ]\n",
    "            # sort in descending order of word counts\n",
    "            word_counts = sorted(word_counts, key=lambda wc: -wc[1])\n",
    "            words = [w[0] for w in word_counts]\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2index[self.PAD_TOKEN] = self.PAD_INDEX\n",
    "        self.word2index[self.SOS_TOKEN] = self.SOS_INDEX\n",
    "        self.word2index[self.EOS_TOKEN] = self.EOS_INDEX\n",
    "        self.word2index[self.UNK_TOKEN] = self.UNK_INDEX\n",
    "        for index, word in enumerate(words):\n",
    "            self.word2index[word] = index + 4\n",
    "\n",
    "        self.index2word = {\n",
    "            index: word for word, index in self.word2index.items()\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_saved(cls, saved_vocabulary_path: str) -> \"Vocabulary\":\n",
    "        \"\"\"Build the vocabulary from a json file saved by ``save`` method.\n",
    "        Parameters\n",
    "        ----------\n",
    "        saved_vocabulary_path : str\n",
    "            Path to a json file containing word to integer mappings\n",
    "            (saved vocabulary).\n",
    "        \"\"\"\n",
    "        with open(saved_vocabulary_path, \"r\") as saved_vocabulary_file:\n",
    "            cls.word2index = json.load(saved_vocabulary_file)\n",
    "        cls.index2word = {\n",
    "            index: word for word, index in cls.word2index.items()\n",
    "        }\n",
    "\n",
    "    def convert_tokens_to_ids(self, words: List[str]) -> List[int]:\n",
    "        return [self.word2index.get(word, self.UNK_INDEX) for word in words]\n",
    "\n",
    "    def convert_ids_to_tokens(self, indices: List[int]) -> List[str]:\n",
    "        return [\n",
    "            self.index2word.get(index, self.UNK_TOKEN) for index in indices\n",
    "        ]\n",
    "\n",
    "    def save(self, save_vocabulary_path: str) -> None:\n",
    "        with open(save_vocabulary_path, \"w\") as save_vocabulary_file:\n",
    "            json.dump(self.word2index, save_vocabulary_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index2word)\n",
    "\n",
    "\n",
    "# in Glove600\n",
    "word_not_founds =  ['<PAD>', '<S>', '</S>', '<UNK>', 'selfie', '20ish', 'babywearing', 'yess', 'idk', 'tannish', 'nno', 'skiis', 'tanish', 'no-', 'pepperonis', 'yrd', 'no*', '*yes', 'noonish', 'nunchuck', '20s-30s', 'yes-', 'goldish', 'frig', 'ywa', 'tshirt', 'yyes', 'yees', 'hahaha', 'yes3', 'frisbe', 'mid-swing', 'yesw', 'yres', 'brocolli', 'yse', 'eys', 'afternoonish', 'silverish', 'trolly', '10ish', 'sorry**', '20-30s', 'yes*', 'yesd', \"'is\", 'kiteboard', 'no**', '30s-40s', 'tell-', \"'no\", 'refridgerator', 'drainer', 'yesa', 'no-but', 'bathmat', 'wakeboarder', 'parasails', 't-ball', \"'t\", 'tee-shirt', 'tyes', 'sandwhich', 'frizbee', 'opps', '25ish', '60ish', 'it-', 'parasailers', 'no-just', 'surfboarding', 'breakroom', 'skii', 'half-eaten', 'deckered', \"'what\", 'lived-in', 'any1', 'selfies', '*no', 'wet-suit', \"'yes\", 'hoody', '7ish', 'parasailer', 'dalmation', 'creamish', 'preforming', 'upclose', 'sorry*', 'surfboarders', 'blck', 'loveseats', 'overcasted', 'coldish', 'mturk', 'ytes', 'buliding', '40s-50s', 'eatable', 'white*', 'passanger', 'mid-jump', 'cruller', 'no-it', 'skort', 'delish', 'so-', 'sorry***', 'blowdryer', 'gnar', 'yellow-ish', 'toliet', 'bedskirt', 'white-fish', 'surfboarder', 'concerte', 'black*', 'white-ish', 'yeds', 'long-hair', 'ohhhhh', 'yws', 'amature', 'yes**', '*i', '*there', 'sortof', 'tell*', 'windsocks', 'cleanish', 'see-thru', 'non-veg', '35ish', 'yes-in', \"30's-40\", 'yes4', 'sorry-', 'peperoni', '9ish', 'freez', 'urnials', 'mid-stride', 'mini-fridge', '5ish', 'yno', 'right-', 'bi-plane', 'stir-fry', 'iwth', 'wakeboards', 'clearish', 'tallish', 'hotplate', 'uhaul', 'doubledecker', '12ish', 'black-', 'gray-blue', 'tanding', 'diffrent', '-but', 'gray-ish', 'footlongs', 'lunchmeat', 'silve', 'danishes', 'knick-knacks', 'barbwire', \"20's-30\", 'multi-colors', 'paperclips', 'inground', 'yes-one', 'multicolored-', 'small-ish', 'papasan', 'woops', 'no4', 'prolly', '*is', 'counter-top', 'waverunner', 'yesl', 'floaties', 'vegitables', 'probaby', 'mayby', 'UNK']\n",
    "\n",
    "# in Glove840B\n",
    "word_not_founds = ['<PAD>', '</S>', '<UNK>', 'no-', 'no*', '*yes', 'yes-', 'ywa', 'yyes', 'yes3', 'frisbe', 'yesw', 'afternoonish', 'sorry**', 'yes*', 'yesd', \"'is\", 'no**', 'tell-', \"'no\", 'yesa', 'no-but', \"'t\", 'it-', 'no-just', 'deckered', \"'what\", '*no', \"'yes\", 'parasailer', 'sorry*', 'surfboarders', 'overcasted', 'white*', 'no-it', 'so-', 'sorry***', 'white-fish', 'surfboarder', 'black*', 'yeds', 'yws', 'yes**', '*i', '*there', 'tell*', 'yes-in', \"30's-40\", 'yes4', 'sorry-', 'urnials', 'right-', 'black-', '-but', \"20's-30\", 'yes-one', 'multicolored-', '*is', 'yesl']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
