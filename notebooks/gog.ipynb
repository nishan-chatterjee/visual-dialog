{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# set the subset of the data to use\n",
    "subset = 1\n",
    "# load the data\n",
    "data = json.load(open('../data/subsets/visdial_1.0_train_' +\n",
    "                 str(subset) + 'percent_subset.json'))['data']\n",
    "# load the dialogs\n",
    "dialogs = data['dialogs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "'''Load the GAP output of the History Graphs Batch'''\n",
    "with open('../embeddings/history/' + str(subset) + '/history_batch_GAP.pkl', 'rb') as f:\n",
    "    history_batch_GAP = pickle.load(f)\n",
    "    \n",
    "'''Load the question graphs'''\n",
    "with open('../embeddings/questions/' + str(subset) + '/question_graphs.pkl', 'rb') as f:\n",
    "    question_graphs = pickle.load(f)\n",
    "\n",
    "'''Load the image graphs'''\n",
    "with open('../embeddings/images/instance/' + str(subset) + '/image_graphs.pkl', 'rb') as f:\n",
    "    image_graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''We now follow a bottom-up approach to build the context aware GoG Graphs. Approach:\n",
    "\n",
    "    Data: <dialogs> contain n entries (depending on the subset) of <10 rounds of dialog> for each image. Each of these <dialog rounds> contain a <question>, an <answer>, <a list of 100 candidate questions>, and a <ground truth index from the candidate questions>\n",
    "    \n",
    "    Task Reminder: The task is to answer the questions in the <dialog round>.\n",
    "\n",
    "    Feature Aggregation:\n",
    "        1 .For each <dialog round>, we obtain 10 history graphs where the first history graph is simply a single node entry of the caption, the second graph being the first history graph + the first question and answer pair, and so on.\n",
    "\n",
    "        For each dialog round, we first aggregate each of the history graphs for this round using GAT and GAP to obtain the embedding representations of the history graphs.\n",
    "        \n",
    "        2. For each <question> in the <dialog round>, we obtain 1 question graph. We use the corresponding History Graph Embedding and concatenate it to every node in the question graph.\n",
    "        Note: The c_in changes to a torch.Size([300+512]) tensor for the GAT.\n",
    "        \n",
    "        Then we perform both GAT and GAp on the question graph to obtain the embedding representation of the question graph.\n",
    "\n",
    "        3. To obtain a context-aware embedding representation of the image, for each question, we obtain the question graph embedding and add it as a node to the image graph. \n",
    "        Note: The c_in changes to a torch.Size([2048+512]) tensor for the GAT.\n",
    "\n",
    "        Then we perform both GAT and GAP on the image graph to obtain the embedding representation of the image graph.\n",
    "\n",
    "        4. Then we use the \"Efficient Attention Mechanism for Visual Dialog that can Handle All the Interactions between Multiple Inputs\" method to train a fusion model using an encoder-decoder (discriminative decoder) to predict the ground truth index from the candidate answers for each question.\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Graph Attention Network (GAT) code from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html'''\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(\n",
    "            num_heads, 2 * c_out))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "        node_feats_flat = node_feats.view(\n",
    "            batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat,\n",
    "                               index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat,\n",
    "                               index=edge_indices_col, dim=0)\n",
    "        ], dim=-1)  # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(\n",
    "            adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(\n",
    "            1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize the GAT layer with:\n",
    "- input dimension of 300+512 = 812 (the dimensionality of the node features)\n",
    "- output dimension of 512 (the dimensionality of the output features)\n",
    "- 4 attention heads\n",
    "- attention heads are not concatenated\n",
    "- alpha is set to 0.2\n",
    "Note: The features here are set from the GoG Paper'''\n",
    "\n",
    "gat_layer_questions = GATLayer(c_in=812, c_out=512, num_heads=4,\n",
    "                     concat_heads=False, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.nn import GlobalAttentionPooling\n",
    "\n",
    "# initialize the gate layer\n",
    "gate_nn = torch.nn.Linear(512, 1)\n",
    "# initialize the GlobalAttentionPooling layer\n",
    "gap = GlobalAttentionPooling(gate_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to create an adjacency matrix from a graph where the input is a list of edges'''\n",
    "\n",
    "def create_adj_matrix(edges, number_of_nodes):\n",
    "    adj_matrix = torch.zeros(number_of_nodes, number_of_nodes)\n",
    "    for edge in edges:\n",
    "        adj_matrix[edge[0], edge[1]] = 1\n",
    "        adj_matrix[edge[1], edge[0]] = 1\n",
    "    # convert to list of lists\n",
    "    adj_matrix = adj_matrix.tolist()\n",
    "    # values should be integers\n",
    "    adj_matrix = [[int(j) for j in i] for i in adj_matrix]\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function that takes in a graph and a node embedding as input and concatenates the node embedding to every node in the graph and returns the updated graph'''\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_node_to_graph(graph, node_embedding):\n",
    "    # concatenate the node embedding to every node in the graph\n",
    "    graph.x = torch.cat((graph.x, node_embedding.repeat(graph.x.shape[0], 1)), 1)\n",
    "    # create the adjacency matrix\n",
    "    adj_matrix = create_adj_matrix(graph.edge_index.T.tolist(), graph.x.shape[0])\n",
    "    # return the updated graph and the adjacency matrix\n",
    "    return graph, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original graph is:  Data(x=[20, 300], edge_index=[2, 10], edge_attr=[6])\n",
      "The updated graph is:  Data(x=[20, 812], edge_index=[2, 10], edge_attr=[6])\n",
      "The updated adjacency matrix shape is:  (20, 20)\n"
     ]
    }
   ],
   "source": [
    "# Testing the add_node_to_graph function\n",
    "temp = question_graphs[0].clone()\n",
    "print('The original graph is: ', temp)\n",
    "temp_update, temp_update_adj = add_node_to_graph(temp, torch.rand(512))\n",
    "print('The updated graph is: ', temp_update)\n",
    "print('The updated adjacency matrix shape is: ',\n",
    "      np.array(temp_update_adj).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [00:30<00:00, 40.48it/s]\n"
     ]
    }
   ],
   "source": [
    "'''For each <dialog> entry, we obtain 10 questions <dialog rounds>'''\n",
    "# store the list of history-aware-question graphs GAP embeddings\n",
    "question_batch_GAP = []\n",
    "\n",
    "for dialog in tqdm(dialogs):\n",
    "\n",
    "    '''Load the history embeddings (GAP) which contains 10 torch tensors of shape 512'''\n",
    "    history_embeddings_dialog = history_batch_GAP[dialogs.index(dialog)]\n",
    "    \n",
    "    '''Load the question indices which correspond to the question graphs'''\n",
    "    question_ids_dialog = [round['question'] for round in dialog['dialog']]\n",
    "    \n",
    "    '''Load the question graphs for this dialog'''\n",
    "    question_graphs_dialog = [question_graphs[i] for i in question_ids_dialog]\n",
    "    \n",
    "    # question GAP embeddings for this dialog\n",
    "    question_GAP_dialog = []\n",
    "\n",
    "    '''For each question graph, we concatenate this to each graph node'''\n",
    "    for h_node, q_graph_dialog in zip(history_embeddings_dialog, question_graphs_dialog):\n",
    "        # clone the question graph\n",
    "        q_graph = q_graph_dialog.clone()\n",
    "        # add the node to the graph\n",
    "        q_graph, adj_matrix = add_node_to_graph(q_graph, h_node)\n",
    "        \n",
    "        '''We pass these graphs and the adjacency matrix through the GAT layer'''\n",
    "        q_graph.x = gat_layer_questions(q_graph.x.unsqueeze(0), torch.tensor([adj_matrix]), print_attn_probs=False).squeeze(0)\n",
    "\n",
    "        '''Now for each question graph, we need to obtain the GAP embedding'''\n",
    "        # initialize an empty DGL graph with the same number of nodes as the question graph\n",
    "        q_graph_GAP = dgl.graph(([], []), num_nodes=q_graph.num_nodes)\n",
    "        # add the node features to the graph where the node feature size is 512\n",
    "        q_graph_GAP_node_feats = q_graph.x\n",
    "        # add the edges to the graph\n",
    "        if q_graph.edge_index.shape[0] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            q_graph_GAP.add_edges(q_graph.edge_index[0], q_graph.edge_index[1])\n",
    "        # apply the GlobalAttentionPooling layer to the graph\n",
    "        q_graph_GAP_embedding = gap(q_graph_GAP, q_graph_GAP_node_feats).squeeze(0)\n",
    "        # append the GAP embedding to the list of GAP embeddings\n",
    "        question_GAP_dialog.append(q_graph_GAP_embedding)\n",
    "\n",
    "    # append the list of GAP embeddings to the list of GAP embeddings for all dialogs\n",
    "    question_batch_GAP.append(question_GAP_dialog)\n",
    "\n",
    "'''Save the question GAP embeddings'''\n",
    "\n",
    "with open('../embeddings/fusion/question_GAP_batch_gog' + str(subset) + '.pkl', 'wb') as f:\n",
    "    pickle.dump(question_batch_GAP, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialize the GAT layer with:\n",
    "- input dimension of 2048+512 (the dimensionality of the node features)\n",
    "- output dimension of 512 (the dimensionality of the output features)\n",
    "- 4 attention heads\n",
    "- attention heads are not concatenated\n",
    "- alpha is set to 0.2\n",
    "Note: The features here are set from the GoG Paper'''\n",
    "\n",
    "gat_layer_images = GATLayer(c_in=2560, c_out=512, num_heads=4,\n",
    "                     concat_heads=False, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1233 [00:00<01:02, 19.60it/s]/tmp/ipykernel_897238/2952753855.py:12: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/aten/src/ATen/native/TensorShape.cpp:2981.)\n",
      "  adj_matrix = create_adj_matrix(graph.edge_index.T.tolist(), graph.x.shape[0])\n",
      "100%|██████████| 1233/1233 [00:45<00:00, 27.18it/s]\n"
     ]
    }
   ],
   "source": [
    "'''For each <dialog> entry, we obtain 10 questions <dialog rounds> for which we need to update the image graph with the history-aware-question GAP embeddings'''\n",
    "# store the list of history-aware-question-aware image graphs GAP embeddings\n",
    "image_batch_GAP = []\n",
    "\n",
    "for dialog in tqdm(dialogs):\n",
    "\n",
    "    '''Load the question embeddings (GAP) which contains 10 torch tensors of shape 512'''\n",
    "    question_embeddings_dialog = question_batch_GAP[dialogs.index(dialog)]\n",
    "    \n",
    "    '''Load the image index which correspond to the image graph'''\n",
    "    image_id_dialog = dialogs.index(dialog)\n",
    "    # Note: In the subset, the image_id_dialog is the same as the dialog_id_dialog that was used to create the image graphs\n",
    "\n",
    "    '''Load the image graphs for this dialog'''\n",
    "    image_graphs_dialog = image_graphs[image_id_dialog]\n",
    "    # since each image will have 10 different contexts, we need to clone the image graph 10 times\n",
    "    image_graphs_dialog = [image_graphs_dialog.clone() for i in range(10)]\n",
    "\n",
    "    # question GAP embeddings for this dialog\n",
    "    image_GAP_dialog = []\n",
    "\n",
    "    '''For each image graph, we concatenate this to each graph node'''\n",
    "    for q_node, i_graph_dialog in zip(question_embeddings_dialog, image_graphs_dialog):\n",
    "        # clone the image graph\n",
    "        i_graph = i_graph_dialog.clone()\n",
    "        # add the node to the graph\n",
    "        i_graph, adj_matrix = add_node_to_graph(i_graph, q_node)\n",
    "\n",
    "        '''We pass these graphs and the adjacency matrix through the GAT layer'''\n",
    "        i_graph.x = gat_layer_images(i_graph.x.unsqueeze(0), torch.tensor(\n",
    "            [adj_matrix]), print_attn_probs=False).squeeze(0)\n",
    "\n",
    "        '''Now for each image graph, we need to obtain the GAP embedding'''\n",
    "        # initialize an empty DGL graph with the same number of nodes as the question graph\n",
    "        i_graph_GAP = dgl.graph(([], []), num_nodes=i_graph.num_nodes)\n",
    "        # add the node features to the graph where the node feature size is 512\n",
    "        i_graph_GAP_node_feats = i_graph.x\n",
    "        # add the edges to the graph\n",
    "        if i_graph.edge_index.shape[0] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            i_graph_GAP.add_edges(i_graph.edge_index[0], i_graph.edge_index[1])\n",
    "        # apply the GlobalAttentionPooling layer to the graph\n",
    "        i_graph_GAP_embedding = gap(\n",
    "            i_graph_GAP, i_graph_GAP_node_feats).squeeze(0)\n",
    "        # append the GAP embedding to the list of GAP embeddings\n",
    "        image_GAP_dialog.append(i_graph_GAP_embedding)\n",
    "\n",
    "    # append the list of GAP embeddings to the list of GAP embeddings for all dialogs\n",
    "    image_batch_GAP.append(image_GAP_dialog)\n",
    "\n",
    "'''Save the image GAP embeddings'''\n",
    "\n",
    "with open('../embeddings/fusion/image_GAP_batch_gog' + str(subset) + '.pkl', 'wb') as f:\n",
    "    pickle.dump(image_batch_GAP, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnnVD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
