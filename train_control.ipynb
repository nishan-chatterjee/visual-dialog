{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# set the subset number to use\n",
    "subset = 1\n",
    "\n",
    "# remove index 880 from all since it was a problem data point that the visdial dataloader ignores for some reason: Check tools/functionalties.ipynb for more details\n",
    "\n",
    "'''GoG IMPLEMENTATION'''\n",
    "\n",
    "'''Load the GAP output of the History Graphs Batch'''\n",
    "with open('../../embeddings/history/' + str(subset) + '/history_batch_GAP.pkl', 'rb') as f:\n",
    "    history_batch_GAP = pickle.load(f)\n",
    "    history_batch_GAP.pop(880)\n",
    "\n",
    "'''Load the GAP output of the Question Graphs Batch'''\n",
    "with open('../../embeddings/fusion/question_GAP_batch_control' + str(subset) + '.pkl', 'rb') as f:\n",
    "    question_GAP_batch = pickle.load(f)\n",
    "    question_GAP_batch.pop(880)\n",
    "\n",
    "'''Load the GAP output of the Image Graphs Batch'''\n",
    "with open('../../embeddings/fusion/image_GAP_batch_control' + str(subset) + '.pkl', 'rb') as f:\n",
    "    image_GAP_batch = pickle.load(f)\n",
    "    image_GAP_batch.pop(880)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the inputs to match the attn_encoder\n",
    "import torch\n",
    "\n",
    "'''Takes in a list of lists of tensors and returns a tensor of tensors: Refactoring the GAP outputs to match the attn_encoder input format'''\n",
    "\n",
    "\n",
    "def adjust_inputs(inputs):\n",
    "    output = []\n",
    "    # flatten the list of lists\n",
    "    inputs = [item for sublist in inputs for item in sublist]\n",
    "    for i in inputs:\n",
    "        i = i.unsqueeze(0)\n",
    "        output.append(i)\n",
    "    output = torch.stack(output)\n",
    "    # convert torch tensor to FloatTensor\n",
    "    output = output.type(torch.FloatTensor)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Input for Attn_Encoder:  torch.Size([80, 1, 512])\n",
      "Question Input for Attn_Encoder:  torch.Size([80, 1, 512])\n",
      "Image Input for Attn_Encoder:  torch.Size([80, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "'''hi: torch.FloatTensor\n",
    "The representation of history utility\n",
    "Shape [batch_size x NH, T, hidden_size]'''\n",
    "hi = DataLoader(adjust_inputs(history_batch_GAP), batch_size=8*10,\n",
    "                shuffle=False)  # note that the batch size is 8 but shaping it to 80 since Dataloader includes the NH dimension\n",
    "\n",
    "'''qe: torch.FloatTensor\n",
    "The representation of question utility\n",
    "Shape [batch_size x NH, N, hidden_size]'''\n",
    "qe = DataLoader(adjust_inputs(question_GAP_batch), batch_size=8*10,\n",
    "                shuffle=False)  # note that the batch size is 8 but shaping it to 80 since Dataloader includes the NH dimension\n",
    "\n",
    "'''im: torch.FloatTensor\n",
    "The representation of image utility\n",
    "Shape [batch_size x NH, K, hidden_size]'''\n",
    "im = DataLoader(adjust_inputs(image_GAP_batch), batch_size=8*10,\n",
    "                shuffle=False)  # note that the batch size is 8 but shaping it to 80 since Dataloader includes the NH dimension\n",
    "\n",
    "for h, q, i in zip(hi, qe, im):\n",
    "    print('History Input for Attn_Encoder: ', h.shape)\n",
    "    print('Question Input for Attn_Encoder: ', q.shape)\n",
    "    print('Image Input for Attn_Encoder: ', i.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Mask for Attn_Encoder:  torch.Size([80, 1])\n",
      "Question Mask for Attn_Encoder:  torch.Size([80, 1])\n",
      "Image Mask for Attn_Encoder:  torch.Size([80, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "'''mask_hi: torch.LongTensor\n",
    "Shape [batch_size x NH, T]'''\n",
    "history_masks = torch.ones(\n",
    "    len(history_batch_GAP)*10, 1)  # 1232 different dialogs x 10 number of history rounds\n",
    "mask_hi = DataLoader(history_masks, batch_size=8*10, shuffle=False)\n",
    "\n",
    "'''mask_qe: torch.LongTensor\n",
    "Shape [batch_size x NH, N]'''\n",
    "question_masks = torch.ones(\n",
    "    len(question_GAP_batch)*10, 1)  # 1232 different dialogs x 10 number of history rounds\n",
    "mask_qe = DataLoader(question_masks, batch_size=8*10, shuffle=False)\n",
    "\n",
    "'''mask_im: torch.LongTensor\n",
    "Shape [batch_size x NH, K]'''\n",
    "image_masks = torch.ones(\n",
    "    len(image_GAP_batch)*10, 1)  # 1232 different dialogs x 10 number of history rounds\n",
    "mask_im = DataLoader(image_masks, batch_size=8*10, shuffle=False)\n",
    "\n",
    "for m_h, m_q, m_i in zip(mask_hi, mask_qe, mask_im):\n",
    "    print('History Mask for Attn_Encoder: ', m_h.shape)\n",
    "    print('Question Mask for Attn_Encoder: ', m_q.shape)\n",
    "    print('Image Mask for Attn_Encoder: ', m_i.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the config from new_options.py\n",
    "config = {'seed': 42,\n",
    "          'dataset': {\n",
    "              'v0.9': False,\n",
    "              'overfit': False,\n",
    "              'concat_hist': False,\n",
    "              'max_seq_len': 20,\n",
    "              'vocab_min_count': 5,\n",
    "              'finetune': False,\n",
    "              'is_add_boundaries': True,\n",
    "              'is_return_options': True,\n",
    "              'num_boxes': 'fixed',\n",
    "              'glove_path': 'datasets/glove/embedding_Glove_840_300d.pkl',\n",
    "              'train_feat_img_path': 'datasets/bottom-up-attention/trainval_resnet101_faster_rcnn_genome__num_boxes_100_100.h5',\n",
    "              'val_feat_img_path': 'datasets/bottom-up-attention/val2018_resnet101_faster_rcnn_genome__num_boxes_100_100.h5',\n",
    "              'test_feat_img_path': 'datasets/bottom-up-attention/test2018_resnet101_faster_rcnn_genome__num_boxes_100_100.h5',\n",
    "              'train_json_dialog_path': 'datasets/annotations/visdial_1.0_train.json',\n",
    "              'val_json_dialog_path': 'datasets/annotations/visdial_1.0_val.json',\n",
    "              'test_json_dialog_path': 'datasets/annotations/visdial_1.0_test.json',\n",
    "              'val_json_dense_dialog_path': 'datasets/annotations/visdial_1.0_val_dense_annotations.json',\n",
    "              'train_json_word_count_path': 'datasets/annotations/visdial_1.0_word_counts_train.json'\n",
    "          },\n",
    "          'model': {\n",
    "              'decoder_type': 'misc',\n",
    "              'encoder_out': ['img', 'ques'],\n",
    "              'hidden_size': 512,\n",
    "              'dropout': 0.1,\n",
    "              'test_mode': False,\n",
    "\n",
    "              # image features\n",
    "              'img_feat_size': 2048,\n",
    "              'img_num_attns': None,\n",
    "              'img_has_bboxes': False,\n",
    "              'img_has_attributes': False,\n",
    "              'img_has_classes': False,\n",
    "\n",
    "              # text features\n",
    "              'txt_vocab_size': 11322,\n",
    "              'txt_tokenizer': 'nlp',\n",
    "              'txt_bidirectional': True,\n",
    "              'txt_embedding_size': 300,\n",
    "              'txt_has_pos_embedding': False,\n",
    "              'txt_has_layer_norm': False,\n",
    "              'txt_has_decoder_layer_norm': False,\n",
    "\n",
    "              # cross attention\n",
    "              'ca_has_shared_attns': False,\n",
    "              'ca_has_proj_linear': False,\n",
    "              'ca_has_layer_norm': False,\n",
    "              'ca_has_residual': False,\n",
    "              'ca_num_attn_stacks': 1,\n",
    "              'ca_num_attn_heads': 4,\n",
    "              'ca_pad_size': 2,\n",
    "              'ca_has_avg_attns': False,\n",
    "              'ca_has_self_attns': False,\n",
    "          },\n",
    "          'solver': {\n",
    "              # Adam optimizer\n",
    "              'optimizer': 'adam',\n",
    "              'adam_betas': [0.9, 0.997],\n",
    "              'adam_eps': 1e-9,\n",
    "              'weight_decay': 1e-5,\n",
    "              'clip_norm': None,\n",
    "              # dataloader\n",
    "              'num_epochs': 100,\n",
    "              'batch_size': 8,\n",
    "              'cpu_workers': 8,\n",
    "              'batch_size_multiplier': 1,\n",
    "              # learning rate scheduler\n",
    "              'scheduler_type': 'LinearLR',\n",
    "              'init_lr': 5e-3,\n",
    "              'min_lr': 1e-5,\n",
    "              'num_samples': 1233,\n",
    "              # warmup scheduler\n",
    "              'warmup_factor': 0.2,\n",
    "              'warmup_epochs': 1,\n",
    "              # linear scheduler\n",
    "              'linear_gama': 0.5,\n",
    "              'milestone_steps': [3, 6, 8, 10, 11],\n",
    "              'fp16': False,\n",
    "          },\n",
    "          'callbacks': {\n",
    "              'resume': False,\n",
    "              'validate': True,\n",
    "              'path_pretrained_ckpt': None,\n",
    "              'save_dir': 'checkpoints/',\n",
    "              'log_dir': 'checkpoints/tensorboard/',\n",
    "          }\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.encoders.attn_encoder import AttentionStack\n",
    "'''The Attention Stack include of 3 blocks (i.e. 9 MHAttentions) to compute the attention from all sources to one target (including itself)\n",
    "Attention from X -> Y and Y -> X can be wrapped into a single MultiHeadAttention\n",
    "And self-attention X -> X: can be wrapped into MultiHeadAttention(X, X)'''\n",
    "\n",
    "# initialize the AttentionStack\n",
    "attention_stack = AttentionStack(config)\n",
    "batch_output = []\n",
    "\n",
    "for i, q, h, m_i, m_q, m_h in zip(im, qe, hi, mask_im, mask_qe, mask_hi):\n",
    "    # convert to tuple\n",
    "    batch_input = (i, q, h, m_i, m_q, m_h)\n",
    "    # pass the inputs to the AttentionStack\n",
    "    batch_output.append(attention_stack(batch_input))\n",
    "    # output : A tuples of the updated representations of inputs as the triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train subset] Tokenizing questions...\n",
      "[train subset] Tokenizing answers...\n",
      "[train subset] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from visdial.data.dataset import VisDialDataset\n",
    "\n",
    "train_dataset = VisDialDataset(config, 'train')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=config['solver']['batch_size'],\n",
    "                              num_workers=config['solver']['cpu_workers'],\n",
    "                              shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "batch_input = {}\n",
    "for batch, i, q, h, m_i, m_q, m_h in zip(train_dataloader, im, qe, hi, mask_im, mask_qe, mask_hi):\n",
    "    # convert to tuple with batch['img_ids'] as the primary key\n",
    "    batch_input[str(batch['img_ids'].tolist())] = (i, q, h, m_i, m_q, m_h)\n",
    "\n",
    "# save them to disk\n",
    "with open('batch_input_control.pkl', 'wb') as f:\n",
    "    pickle.dump(batch_input, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnVD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0f314d012c094e500b437d772ea9d63f13832a9dbf30d5ab8fe744ae8c413d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
